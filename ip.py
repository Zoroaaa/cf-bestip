# æ–‡ä»¶2: ip_scanner.py (ä¿®æ”¹åç‰ˆæœ¬)
import subprocess
import random
import ipaddress
import requests
import os
import json
import time
import logging
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime

from proxy_sources import (
    ProxyInfo,
    fetch_proxifly_proxies,
    fetch_proxydaily_proxies,
    fetch_tomcat1235_proxies,
    fetch_hookzof_proxies,
    fetch_proxyscrape_proxies
)

# =========================
# é…ç½®æ—¥å¿—
# =========================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[logging.StreamHandler()]
)

# =========================
# åŸºç¡€å‚æ•°
# =========================

CF_IPS_V4_URL = "https://www.cloudflare.com/ips-v4"

TRACE_DOMAINS = {
    "v0": "sptest.ittool.pp.ua",
    "v1": "sptest1.ittool.pp.ua",
    "v2": "sptest2.ittool.pp.ua",
}

SAMPLE_SIZE = 600
TIMEOUT = 15
CONNECT_TIMEOUT = 5
MAX_WORKERS = 20
LATENCY_LIMIT = 1300

OUTPUT_DIR = "public"
DATA_DIR = "public/data"

HTTPS_PORTS = [443, 8443, 2053, 2083, 2087, 2096]

# ä»£ç†æ£€æµ‹ API é…ç½®
PROXY_CHECK_API_URL = "https://prcheck.ittool.pp.ua/check"  # å¡«å…¥ä½ çš„ API åœ°å€,ä¾‹å¦‚: https://your-worker.workers.dev/check
PROXY_CHECK_API_TOKEN = "588wbb"  # å¡«å…¥ä½ çš„ API Token

# ç›®æ ‡åœ°åŒºé…ç½®
REGION_CONFIG = {
    "HK": {"codes": ["HK"], "sample": 60},
    "SG": {"codes": ["SG"], "sample": 60},
    "JP": {"codes": ["JP"], "sample": 60},
    "KR": {"codes": ["KR"], "sample": 60},
    "TW": {"codes": ["TW"], "sample": 60},
    "US": {"codes": ["US"], "sample": 60},
    "DE": {"codes": ["DE"], "sample": 60},
    "UK": {"codes": ["GB"], "sample": 60},
    "AU": {"codes": ["AU"], "sample": 60},
    "CA": {"codes": ["CA"], "sample": 60},
}

MAX_OUTPUT_PER_REGION = 6
MAX_PROXIES_PER_REGION = 5

# ä»£ç†æµ‹è¯•é…ç½®
PROXY_TEST_TIMEOUT = 10
PROXY_MAX_LATENCY = 1500  # SOCKS5 å’Œ HTTPS ä»£ç†çš„æœ€å¤§å»¶è¿Ÿ
SOCKS5_MAX_LATENCY = 1500  # SOCKS5 ä¸“ç”¨å»¶è¿Ÿé™åˆ¶

# =========================
# COLO â†’ Region æ˜ å°„
# =========================

COLO_MAP = {
    "HKG": "HK", "SIN": "SG", "NRT": "JP", "KIX": "JP",
    "ICN": "KR", "TPE": "TW",
    "SYD": "AU", "MEL": "AU",
    "LAX": "US", "SJC": "US", "SFO": "US",
    "SEA": "US", "ORD": "US", "DFW": "US",
    "ATL": "US", "IAD": "US", "EWR": "US",
    "JFK": "US", "BOS": "US", "MIA": "US",
    "PHX": "US", "DEN": "US", "IAH": "US",
    "FRA": "DE", "MUC": "DE", "AMS": "DE",
    "LHR": "UK", "LGW": "UK", "MAN": "UK",
    "YYZ": "CA", "YVR": "CA",
}

# å›½å®¶ä»£ç åˆ°åœ°åŒºçš„æ˜ å°„(ç”¨äºå¤„ç†æœªåŒ¹é…çš„ä»£ç†åœ°åŒº)
COUNTRY_TO_REGION = {
    "HK": "HK", "SG": "SG", "JP": "JP", "KR": "KR", "TW": "TW",
    "US": "US", "DE": "DE", "GB": "UK", "AU": "AU", "CA": "CA",
    "FR": "DE", "NL": "DE", "IT": "DE", "ES": "DE",  # æ¬§æ´²å…¶ä»–å›½å®¶å½’å…¥DE
    "BR": "US", "MX": "US", "AR": "US",  # ç¾æ´²å…¶ä»–å›½å®¶å½’å…¥US
    "IN": "SG", "TH": "SG", "ID": "SG", "MY": "SG",  # äºšæ´²å…¶ä»–å›½å®¶å½’å…¥SG
}

# =========================
# æ•°æ®æºé…ç½®
# =========================

REGION_TO_COUNTRY_CODE = {
    "HK": "HK", "SG": "SG", "JP": "JP", "KR": "KR", "TW": "TW",
    "US": "US", "DE": "DE", "UK": "GB", "AU": "AU", "CA": "CA",
}

# =========================
# ä½¿ç”¨APIæ£€æµ‹ä»£ç†
# =========================

def check_proxy_with_api(proxy_info):
    """ä½¿ç”¨APIæ£€æµ‹ä»£ç†çš„å¯ç”¨æ€§å’Œä¿¡æ¯"""
    if not PROXY_CHECK_API_URL:
        logging.error("æœªé…ç½® PROXY_CHECK_API_URL,æ— æ³•æ£€æµ‹ä»£ç†")
        return {"success": False, "latency": 999999}
    
    # æ„é€ ä»£ç†URL
    if proxy_info.type in ["socks5", "socks4"]:
        proxy_url = f"socks5://{proxy_info.host}:{proxy_info.port}"
    else:
        proxy_url = f"http://{proxy_info.host}:{proxy_info.port}"
    
    start = time.time()
    
    try:
        params = {"proxy": proxy_url}
        if PROXY_CHECK_API_TOKEN:
            params["token"] = PROXY_CHECK_API_TOKEN
        
        response = requests.get(
            PROXY_CHECK_API_URL,
            params=params,
            timeout=PROXY_TEST_TIMEOUT + 2
        )
        
        latency = int((time.time() - start) * 1000)
        
        if response.status_code != 200:
            return {"success": False, "latency": 999999, "https_ok": False}
        
        result = response.json()
        
        if not result.get("success"):
            return {"success": False, "latency": 999999, "https_ok": False}
        
        # ä»APIç»“æœä¸­æå–ä¿¡æ¯
        location = result.get("location", {})
        country_code = location.get("country_code", "UNKNOWN")
        
        # æ›´æ–°ä»£ç†ä¿¡æ¯
        if proxy_info.country_code == "UNKNOWN":
            proxy_info.country_code = country_code
        
        proxy_info.api_result = result
        
        # æ ¹æ®ä»£ç†ç±»å‹åº”ç”¨å»¶è¿Ÿé™åˆ¶
        max_latency = SOCKS5_MAX_LATENCY if proxy_info.type == "socks5" else PROXY_MAX_LATENCY
        
        if latency > max_latency:
            return {"success": False, "latency": latency, "https_ok": False}
        
        proxy_info.tested_latency = latency
        proxy_info.https_ok = True
        
        return {
            "success": True,
            "latency": latency,
            "https_ok": True,
            "country_code": country_code
        }
        
    except Exception as e:
        logging.debug(f"ä»£ç† {proxy_info.host}:{proxy_info.port} APIæ£€æµ‹å¤±è´¥: {e}")
        return {"success": False, "latency": 999999, "https_ok": False}

# =========================
# è·å–è¯¥åœ°åŒºçš„æœ€ä½³ä»£ç†
# =========================

def get_proxies(region):
    """è·å–æŒ‡å®šåœ°åŒºçš„æœ€ä½³ä»£ç†(å¤šæ•°æ®æºèšåˆ)"""
    all_proxies = []
    
    # æ•°æ®æº 1: Proxifly
    proxifly_proxies = fetch_proxifly_proxies(region, REGION_TO_COUNTRY_CODE)
    all_proxies.extend(proxifly_proxies)
    
    # æ•°æ®æº 2: ProxyDaily
    proxydaily_proxies = fetch_proxydaily_proxies(region, REGION_TO_COUNTRY_CODE, max_pages=2)
    all_proxies.extend(proxydaily_proxies)
    
    # æ•°æ®æº 3: Tomcat1235
    tomcat_proxies = fetch_tomcat1235_proxies(region)
    all_proxies.extend(tomcat_proxies)
    
    # æ•°æ®æº 4: Hookzof
    hookzof_proxies = fetch_hookzof_proxies(region)
    all_proxies.extend(hookzof_proxies)
    
    # æ•°æ®æº 5: Proxyscrape
    proxyscrape_proxies = fetch_proxyscrape_proxies(region, REGION_TO_COUNTRY_CODE)
    all_proxies.extend(proxyscrape_proxies)
    
    # åœ°åŒºè¿‡æ»¤å’Œæ˜ å°„
    target_country_code = REGION_TO_COUNTRY_CODE.get(region, region.upper())
    filtered_proxies = []
    
    for proxy in all_proxies:
        # ç›´æ¥åŒ¹é…
        if proxy.country_code == target_country_code:
            filtered_proxies.append(proxy)
            continue
        
        # é€šè¿‡æ˜ å°„åŒ¹é…
        mapped_region = COUNTRY_TO_REGION.get(proxy.country_code)
        if mapped_region == region:
            filtered_proxies.append(proxy)
            continue
    
    if not filtered_proxies:
        logging.warning(f"âš  {region} æ— åŒ¹é…çš„ä»£ç†,å°è¯•ä½¿ç”¨æ‰€æœ‰å¯ç”¨ä»£ç†")
        filtered_proxies = all_proxies
    
    logging.info(f"{region} å…±æ”¶é›† {len(filtered_proxies)} ä¸ªä»£ç†(æ¥è‡ª {len(all_proxies)} ä¸ªåŸå§‹ä»£ç†)")
    
    if not filtered_proxies:
        logging.warning(f"âš  {region} æ— å¯ç”¨ä»£ç†,å°†å®Œå…¨ä½¿ç”¨ç›´è¿")
        return []
    
    # é™åˆ¶æµ‹è¯•æ•°é‡(ä¼˜å…ˆ SOCKS5)
    socks5_proxies = [p for p in filtered_proxies if p.type == "socks5"]
    https_proxies = [p for p in filtered_proxies if p.type == "https"]
    
    test_proxies = (socks5_proxies[:30] + https_proxies[:30])[:50]
    
    logging.info(f"{region} æµ‹è¯• {len(test_proxies)} ä¸ªä»£ç† (SOCKS5: {len([p for p in test_proxies if p.type == 'socks5'])}, HTTPS: {len([p for p in test_proxies if p.type == 'https'])})")
    
    # å¹¶å‘æµ‹è¯•
    candidate_proxies = []
    
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_proxy = {executor.submit(check_proxy_with_api, p): p for p in test_proxies}
        
        for future in as_completed(future_to_proxy):
            proxy = future_to_proxy[future]
            try:
                test_result = future.result()
                if test_result["success"]:
                    candidate_proxies.append(proxy)
            except Exception as e:
                logging.debug(f"ä»£ç†æµ‹è¯•å¼‚å¸¸: {e}")
    
    if not candidate_proxies:
        logging.warning(f"âš  {region} æ— å¯ç”¨ä»£ç†,å°†å®Œå…¨ä½¿ç”¨ç›´è¿")
        return []
    
    logging.info(f"  âœ“ é€šè¿‡: {len(candidate_proxies)} ä¸ªä»£ç†")
    
    # æŒ‰åè®®å’Œå»¶è¿Ÿæ’åº(SOCKS5 ä¼˜å…ˆ)
    socks5_list = [p for p in candidate_proxies if p.type == "socks5"]
    https_list = [p for p in candidate_proxies if p.type == "https"]
    
    socks5_list.sort(key=lambda x: x.tested_latency)
    https_list.sort(key=lambda x: x.tested_latency)
    
    # ç»„åˆ:ä¼˜å…ˆ SOCKS5
    best_proxies = socks5_list[:MAX_PROXIES_PER_REGION]
    remaining = MAX_PROXIES_PER_REGION - len(best_proxies)
    if remaining > 0:
        best_proxies.extend(https_list[:remaining])
    
    logging.info(f"âœ“ {region} æœ€ç»ˆé€‰å‡º {len(best_proxies)} ä¸ªä»£ç†:")
    for i, p in enumerate(best_proxies, 1):
        logging.info(f"  {i}. {p.host}:{p.port} ({p.type.upper()}) - å»¶è¿Ÿ:{p.tested_latency}ms [src:{p.source}]")
    
    return best_proxies

# =========================
# IP æµ‹è¯•å‡½æ•°
# =========================

def curl_test_with_proxy(ip, domain, proxy=None):
    """ä½¿ç”¨ä»£ç†æµ‹è¯• Cloudflare IP"""
    try:
        cmd = ["curl", "-k", "-o", "/dev/null", "-s"]
        
        if proxy:
            if proxy.type in ['socks5', 'socks4']:
                cmd.extend(["--socks5", f"{proxy.host}:{proxy.port}"])
            else:
                cmd.extend(["-x", f"http://{proxy.host}:{proxy.port}"])
        
        cmd.extend([
            "-w", "%{time_connect} %{time_appconnect} %{http_code}",
            "--http1.1",
            "--connect-timeout", str(CONNECT_TIMEOUT + 2),
            "--max-time", str(TIMEOUT + 3),
            "--resolve", f"{domain}:443:{ip}",
            f"https://{domain}"
        ])
        
        out = subprocess.check_output(cmd, timeout=TIMEOUT + 5, stderr=subprocess.DEVNULL)
        parts = out.decode().strip().split()
        
        if len(parts) < 3:
            return None
        
        tc, ta, code = parts[0], parts[1], parts[2]
        
        if code in ["000", "0"]:
            return None
        
        latency = int((float(tc) + float(ta)) * 1000)
        
        if latency > LATENCY_LIMIT:
            return None
        
        # è·å– CF-Ray
        hdr_cmd = ["curl", "-k", "-sI"]
        
        if proxy:
            if proxy.type in ['socks5', 'socks4']:
                hdr_cmd.extend(["--socks5", f"{proxy.host}:{proxy.port}"])
            else:
                hdr_cmd.extend(["-x", f"http://{proxy.host}:{proxy.port}"])
        
        hdr_cmd.extend([
            "--connect-timeout", str(CONNECT_TIMEOUT + 2),
            "--max-time", str(TIMEOUT + 3),
            "--resolve", f"{domain}:443:{ip}",
            f"https://{domain}"
        ])
        
        hdr = subprocess.check_output(
            hdr_cmd,
            timeout=TIMEOUT + 3,
            stderr=subprocess.DEVNULL
        ).decode(errors="ignore").lower()
        
        ray = None
        for line in hdr.splitlines():
            if line.startswith("cf-ray"):
                ray = line.split(":")[1].strip()
                break
        
        if not ray:
            return None
        
        colo = ray.split("-")[-1].upper()
        region = COLO_MAP.get(colo, "UNMAPPED")
        
        return {
            "ip": str(ip),
            "domain": domain,
            "colo": colo,
            "region": region,
            "latency": latency,
            "proxy": f"{proxy.host}:{proxy.port}({proxy.type})" if proxy else "direct"
        }
        
    except subprocess.TimeoutExpired:
        return None
    except Exception as e:
        logging.debug(f"æµ‹è¯•å¤±è´¥: {ip} - {e}")
        return None

def test_ip_with_proxy(ip, proxy=None):
    records = []
    for view, domain in TRACE_DOMAINS.items():
        r = curl_test_with_proxy(ip, domain, proxy)
        if r:
            r["view"] = view
            records.append(r)
    return records

# =========================
# å·¥å…·å‡½æ•°
# =========================

def fetch_cf_ipv4_cidrs():
    r = requests.get(CF_IPS_V4_URL, timeout=10)
    r.raise_for_status()
    return [x.strip() for x in r.text.splitlines() if x.strip()]

def weighted_random_ips(cidrs, total):
    pools = []
    for c in cidrs:
        net = ipaddress.ip_network(c)
        pools.append((net, net.num_addresses))
    
    total_weight = sum(w for _, w in pools)
    result = []
    
    for net, weight in pools:
        cnt = max(1, int(total * weight / total_weight))
        hosts = list(net.hosts())
        if hosts:
            result.extend(random.sample(hosts, min(cnt, len(hosts))))
    
    random.shuffle(result)
    return result[:total]

def score_ip(latencies):
    if len(latencies) < 2:
        return 0
    
    lat_min = min(latencies)
    lat_max = max(latencies)
    
    s_stability = len(latencies) / len(TRACE_DOMAINS)
    s_consistency = max(0.3, 1 - (lat_max - lat_min) / LATENCY_LIMIT)
    s_latency = 1 / (1 + lat_min / 100)
    
    return round(s_stability * s_consistency * s_latency, 4)

def aggregate_nodes(raw):
    ip_map = defaultdict(list)
    for r in raw:
        ip_map[r["ip"]].append(r)
    
    nodes = []
    for ip, items in ip_map.items():
        latencies = [x["latency"] for x in items]
        score = score_ip(latencies)
        if score <= 0:
            continue
        
        best = min(items, key=lambda x: x["latency"])
        nodes.append({
            "ip": ip,
            "port": random.choice(HTTPS_PORTS),
            "region": best["region"],
            "colo": best["colo"],
            "latencies": latencies,
            "score": score
        })
    
    return nodes

# =========================
# åˆ†åœ°åŒºæ‰«æ
# =========================

def scan_region(region, ips, proxies):
    logging.info(f"\n{'='*60}")
    logging.info(f"å¼€å§‹æ‰«æåœ°åŒº: {region}")
    logging.info(f"{'='*60}")
    
    raw_results = []
    
    if proxies:
        logging.info(f"ä½¿ç”¨ {len(proxies)} ä¸ªä»£ç†è¿›è¡Œæ‰«æ...")
        
        ips_per_proxy = max(1, len(ips) // len(proxies))
        
        for i, proxy in enumerate(proxies):
            proxy_ips = ips[i*ips_per_proxy:(i+1)*ips_per_proxy]
            
            if not proxy_ips:
                continue
            
            proxy_info = f"{proxy.host}:{proxy.port}({proxy.type})"
            logging.info(f"  â†’ é€šè¿‡ä»£ç† {proxy_info} æµ‹è¯• {len(proxy_ips)} ä¸ªIP...")
            
            with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
                futures = [executor.submit(test_ip_with_proxy, ip, proxy) for ip in proxy_ips]
                
                for future in as_completed(futures):
                    try:
                        batch = future.result(timeout=TIMEOUT + 5)
                        if batch:
                            raw_results.extend(batch)
                    except:
                        pass
        
        logging.info(f"  âœ“ ä»£ç†æ‰«ææ”¶é›†: {len(raw_results)} æ¡ç»“æœ")
    
    # åŠ¨æ€è¡¥å……ç­–ç•¥
    expected_results = len(ips) * 0.2
    
    if len(raw_results) < expected_results:
        supplement_count = len(ips) // 2 if raw_results else len(ips)
        logging.info(f"âš  ä»£ç†ç»“æœä¸è¶³({len(raw_results)}/{expected_results:.0f}),ä½¿ç”¨ç›´è¿è¡¥å…… {supplement_count} ä¸ªIP...")
        
        remaining_ips = ips[:supplement_count]
        
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            futures = [executor.submit(test_ip_with_proxy, ip, None) for ip in remaining_ips]
            
            for future in as_completed(futures):
                try:
                    batch = future.result(timeout=TIMEOUT + 5)
                    if batch:
                        raw_results.extend(batch)
                except:
                    pass
        
        logging.info(f"  âœ“ ç›´è¿è¡¥å……æ”¶é›†,å½“å‰æ€»è®¡: {len(raw_results)} æ¡ç»“æœ")
    else:
        logging.info(f"  âœ“ ä»£ç†ç»“æœå……è¶³,è·³è¿‡ç›´è¿è¡¥å……")
    
    logging.info(f"âœ“ {region}: æ€»è®¡æ”¶é›† {len(raw_results)} æ¡æµ‹è¯•ç»“æœ\n")
    return raw_results

# =========================
# å†…éƒ¨æµ‹è¯•å‡½æ•°
# =========================

def run_internal_tests():
    """è¿è¡Œå†…éƒ¨å¯ç”¨æ€§æµ‹è¯•"""
    logging.info("\n" + "="*60)
    logging.info("å¼€å§‹å†…éƒ¨æµ‹è¯•...")
    logging.info("="*60)
    
    test_results = {
        "data_sources": {},
        "proxy_tests": {},
        "api_check": None,
        "cf_ip_fetch": None
    }
    
    # æµ‹è¯• 1: Cloudflare IP æ®µè·å–
    logging.info("\n[æµ‹è¯• 1/5] Cloudflare IP æ®µè·å–...")
    try:
        cidrs = fetch_cf_ipv4_cidrs()
        if len(cidrs) > 0:
            logging.info(f"  âœ“ æˆåŠŸè·å– {len(cidrs)} ä¸ª IP æ®µ")
            test_results["cf_ip_fetch"] = True
        else:
            logging.error("  âœ— IP æ®µåˆ—è¡¨ä¸ºç©º")
            test_results["cf_ip_fetch"] = False
    except Exception as e:
        logging.error(f"  âœ— è·å–å¤±è´¥: {e}")
        test_results["cf_ip_fetch"] = False
    
    # æµ‹è¯• 2: æ•°æ®æºæµ‹è¯•
    logging.info("\n[æµ‹è¯• 2/5] ä»£ç†æ•°æ®æºæµ‹è¯•...")
    test_region = "US"
    
    # Proxifly
    logging.info("  æµ‹è¯• Proxifly...")
    try:
        proxifly_list = fetch_proxifly_proxies(test_region, REGION_TO_COUNTRY_CODE)
        test_results["data_sources"]["proxifly"] = len(proxifly_list) > 0
        logging.info(f"    âœ“ Proxifly: {len(proxifly_list)} ä¸ªä»£ç†")
    except Exception as e:
        test_results["data_sources"]["proxifly"] = False
        logging.error(f"    âœ— Proxifly å¤±è´¥: {e}")
    
    # ProxyDaily
    logging.info("  æµ‹è¯• ProxyDaily...")
    try:
        proxydaily_list = fetch_proxydaily_proxies(test_region, REGION_TO_COUNTRY_CODE, max_pages=1)
        test_results["data_sources"]["proxydaily"] = len(proxydaily_list) > 0
        logging.info(f"    âœ“ ProxyDaily: {len(proxydaily_list)} ä¸ªä»£ç†")
    except Exception as e:
        test_results["data_sources"]["proxydaily"] = False
        logging.error(f"    âœ— ProxyDaily å¤±è´¥: {e}")
    
    # Tomcat1235
    logging.info("  æµ‹è¯• Tomcat1235...")
    try:
        tomcat_list = fetch_tomcat1235_proxies(test_region)
        test_results["data_sources"]["tomcat1235"] = len(tomcat_list) > 0
        logging.info(f"    âœ“ Tomcat1235: {len(tomcat_list)} ä¸ªä»£ç†")
    except Exception as e:
        test_results["data_sources"]["tomcat1235"] = False
        logging.error(f"    âœ— Tomcat1235 å¤±è´¥: {e}")
    
    # Hookzof
    logging.info("  æµ‹è¯• Hookzof...")
    try:
        hookzof_list = fetch_hookzof_proxies(test_region)
        test_results["data_sources"]["hookzof"] = len(hookzof_list) > 0
        logging.info(f"    âœ“ Hookzof: {len(hookzof_list)} ä¸ªä»£ç†")
    except Exception as e:
        test_results["data_sources"]["hookzof"] = False
        logging.error(f"    âœ— Hookzof å¤±è´¥: {e}")
    
    # Proxyscrape
    logging.info("  æµ‹è¯• Proxyscrape...")
    try:
        proxyscrape_list = fetch_proxyscrape_proxies(test_region, REGION_TO_COUNTRY_CODE)
        test_results["data_sources"]["proxyscrape"] = len(proxyscrape_list) > 0
        logging.info(f"    âœ“ Proxyscrape: {len(proxyscrape_list)} ä¸ªä»£ç†")
    except Exception as e:
        test_results["data_sources"]["proxyscrape"] = False
        logging.error(f"    âœ— Proxyscrape å¤±è´¥: {e}")
    
    # æµ‹è¯• 3: API å¯ç”¨æ€§æµ‹è¯•
    logging.info("\n[æµ‹è¯• 3/5] ä»£ç†æ£€æµ‹ API æµ‹è¯•...")
    if not PROXY_CHECK_API_URL:
        logging.warning("  âš  æœªé…ç½® PROXY_CHECK_API_URL,è·³è¿‡APIæµ‹è¯•")
        test_results["api_check"] = False
    else:
        try:
            # ä½¿ç”¨ä¸€ä¸ªå…¬å…±ä»£ç†æµ‹è¯•API
            test_proxy = ProxyInfo("8.8.8.8", 1080, "socks5", source="test")
            result = check_proxy_with_api(test_proxy)
            if result.get("success") or "latency" in result:
                logging.info("  âœ“ API å“åº”æ­£å¸¸")
                test_results["api_check"] = True
            else:
                logging.warning("  âš  API å“åº”å¼‚å¸¸")
                test_results["api_check"] = False
        except Exception as e:
            logging.error(f"  âœ— API æµ‹è¯•å¤±è´¥: {e}")
            test_results["api_check"] = False
    
    # æµ‹è¯• 4: ä»£ç†è¿é€šæ€§æµ‹è¯•
    logging.info("\n[æµ‹è¯• 4/5] ä»£ç†è¿é€šæ€§æµ‹è¯•...")
    
    # æ”¶é›†ä¸€äº›æµ‹è¯•ä»£ç†
    all_test_proxies = []
    if test_results["data_sources"].get("proxifly"):
        all_test_proxies.extend(proxifly_list[:3])
    if test_results["data_sources"].get("proxydaily"):
        all_test_proxies.extend(proxydaily_list[:3])
    if test_results["data_sources"].get("hookzof"):
        all_test_proxies.extend(hookzof_list[:3])
    if test_results["data_sources"].get("proxyscrape"):
        all_test_proxies.extend(proxyscrape_list[:3])
    
    if all_test_proxies and PROXY_CHECK_API_URL:
        logging.info(f"  æµ‹è¯• {len(all_test_proxies)} ä¸ªä»£ç†...")
        working_proxies = 0
        
        for proxy in all_test_proxies[:5]:  # æœ€å¤šæµ‹è¯•5ä¸ª
            result = check_proxy_with_api(proxy)
            if result["success"]:
                working_proxies += 1
                logging.info(f"    âœ“ {proxy.host}:{proxy.port} ({proxy.type}) - {result['latency']}ms")
        
        test_results["proxy_tests"]["working_count"] = working_proxies
        test_results["proxy_tests"]["total_tested"] = len(all_test_proxies[:5])
        
        if working_proxies > 0:
            logging.info(f"  âœ“ {working_proxies}/{len(all_test_proxies[:5])} ä¸ªä»£ç†å¯ç”¨")
        else:
            logging.warning("  âš  æ²¡æœ‰å¯ç”¨ä»£ç†")
    else:
        logging.warning("  âš  æ— ä»£ç†å¯æµ‹è¯•æˆ–APIæœªé…ç½®")
        test_results["proxy_tests"]["working_count"] = 0
        test_results["proxy_tests"]["total_tested"] = 0
    
    # æµ‹è¯• 5: CF IP æµ‹è¯•
    logging.info("\n[æµ‹è¯• 5/5] Cloudflare IP æµ‹è¯•...")
    try:
        test_ips = weighted_random_ips(cidrs, 5)
        logging.info(f"  æµ‹è¯• {len(test_ips)} ä¸ª Cloudflare IP...")
        
        test_ip = test_ips[0]
        result = curl_test_with_proxy(test_ip, "sptest.ittool.pp.ua", None)
        
        if result:
            logging.info(f"    âœ“ æµ‹è¯•æˆåŠŸ: {result['ip']} -> {result['region']} ({result['latency']}ms)")
            test_results["cf_ip_test"] = True
        else:
            logging.warning("    âš  CF IP æµ‹è¯•æœªè¿”å›ç»“æœ")
            test_results["cf_ip_test"] = False
    except Exception as e:
        logging.error(f"  âœ— CF IP æµ‹è¯•å¤±è´¥: {e}")
        test_results["cf_ip_test"] = False
    
    # æµ‹è¯•æ€»ç»“
    logging.info("\n" + "="*60)
    logging.info("æµ‹è¯•æ€»ç»“")
    logging.info("="*60)
    
    passed_tests = 0
    total_tests = 0
    
    # CF IP æ®µ
    total_tests += 1
    if test_results["cf_ip_fetch"]:
        logging.info("âœ“ Cloudflare IP æ®µè·å–: é€šè¿‡")
        passed_tests += 1
    else:
        logging.error("âœ— Cloudflare IP æ®µè·å–: å¤±è´¥")
    
    # æ•°æ®æº
    for source, status in test_results["data_sources"].items():
        total_tests += 1
        if status:
            logging.info(f"âœ“ æ•°æ®æº {source}: é€šè¿‡")
            passed_tests += 1
        else:
            logging.warning(f"âš  æ•°æ®æº {source}: å¤±è´¥(éè‡´å‘½)")
    
    # API æ£€æµ‹
    total_tests += 1
    if test_results["api_check"]:
        logging.info("âœ“ ä»£ç†æ£€æµ‹ API: é€šè¿‡")
        passed_tests += 1
    else:
        logging.warning("âš  ä»£ç†æ£€æµ‹ API: æœªé…ç½®æˆ–å¤±è´¥(éè‡´å‘½)")
    
    # ä»£ç†æµ‹è¯•
    total_tests += 1
    proxy_working = test_results["proxy_tests"].get("working_count", 0)
    if proxy_working > 0:
        logging.info(f"âœ“ ä»£ç†è¿é€šæ€§: é€šè¿‡ ({proxy_working} ä¸ªå¯ç”¨)")
        passed_tests += 1
    else:
        logging.warning("âš  ä»£ç†è¿é€šæ€§: æ— å¯ç”¨ä»£ç†(å°†ä½¿ç”¨ç›´è¿)")
    
    # CF IP æµ‹è¯•
    total_tests += 1
    if test_results.get("cf_ip_test"):
        logging.info("âœ“ CF IP æµ‹è¯•: é€šè¿‡")
        passed_tests += 1
    else:
        logging.error("âœ— CF IP æµ‹è¯•: å¤±è´¥")
    
    logging.info("="*60)
    logging.info(f"æµ‹è¯•ç»“æœ: {passed_tests}/{total_tests} é€šè¿‡")
    
    if passed_tests >= total_tests - 2:  # å…è®¸æœ€å¤š2ä¸ªéå…³é”®æµ‹è¯•å¤±è´¥
        logging.info("âœ… ç³»ç»Ÿå¯ç”¨æ€§æµ‹è¯•é€šè¿‡,å¯ä»¥å¼€å§‹æ‰«æ")
        return True
    else:
        logging.error("âŒ ç³»ç»Ÿå¯ç”¨æ€§æµ‹è¯•å¤±è´¥,è¯·æ£€æŸ¥ç½‘ç»œå’Œä¾èµ–")
        return False

# =========================
# ä¿å­˜ä»£ç†åˆ—è¡¨
# =========================

def save_proxy_list(region_proxies):
    """ä¿å­˜æ‰€æœ‰å¯ç”¨ä»£ç†åˆ°txtæ–‡ä»¶"""
    all_proxies_lines = []
    
    for region, proxies in region_proxies.items():
        for proxy in proxies:
            # æ ¼å¼: ip:port#REGION_å»¶è¿Ÿ_æ¥æºä»£ç†æ± 
            line = f"{proxy.host}:{proxy.port}#{region}_{proxy.tested_latency}ms_{proxy.source}\n"
            all_proxies_lines.append(line)
    
    # ä¿å­˜æ€»ä»£ç†åˆ—è¡¨
    with open(f"{OUTPUT_DIR}/proxy_all.txt", "w") as f:
        f.writelines(all_proxies_lines)
    
    logging.info(f"âœ“ ä¿å­˜ä»£ç†åˆ—è¡¨: {len(all_proxies_lines)} ä¸ªä»£ç† -> {OUTPUT_DIR}/proxy_all.txt")
    
    # æŒ‰åœ°åŒºä¿å­˜
    for region, proxies in region_proxies.items():
        lines = []
        for proxy in proxies:
            line = f"{proxy.host}:{proxy.port}#{region}_{proxy.tested_latency}ms_{proxy.source}\n"
            lines.append(line)
        
        with open(f"{OUTPUT_DIR}/proxy_{region}.txt", "w") as f:
            f.writelines(lines)
        
        logging.info(f"  {region}: {len(lines)} ä¸ªä»£ç†")

# =========================
# åŠ è½½HTMLæ¨¡æ¿
# =========================

def load_html_template():
    """ä»ç‹¬ç«‹æ–‡ä»¶åŠ è½½HTMLæ¨¡æ¿"""
    template_path = os.path.join(os.path.dirname(__file__), 'template.html')
    with open(template_path, 'r', encoding='utf-8') as f:
        return f.read()

# =========================
# ç”ŸæˆHTMLé¡µé¢
# =========================

def generate_html(all_nodes, region_results, region_proxies):
    """ç”ŸæˆHTMLå±•ç¤ºé¡µé¢"""
    template = load_html_template()
    
    # ç”Ÿæˆåœ°åŒºå¡ç‰‡
    region_cards_html = []
    
    for region in sorted(region_results.keys()):
        nodes = region_results[region]
        if not nodes:
            continue
        
        # æ¯ä¸ªåœ°åŒºçš„IPåˆ—è¡¨
        ip_items_html = []
        for node in nodes[:MAX_OUTPUT_PER_REGION]:
            min_latency = min(node['latencies'])
            ip_html = f"""
            <div class="ip-item">
                <div class="ip-address">{node['ip']}:{node['port']}</div>
                <div class="ip-meta">
                    <span class="badge badge-score">åˆ†æ•° {node['score']}</span>
                    <span class="badge badge-latency">å»¶è¿Ÿ {min_latency}ms</span>
                    <span class="badge badge-colo">COLO {node['colo']}</span>
                </div>
            </div>"""
            ip_items_html.append(ip_html)
        
        # æ¯ä¸ªåœ°åŒºçš„ä»£ç†åˆ—è¡¨ (æ–°å¢æŸ¥é˜…åŠŸèƒ½)
        proxy_items_html = []
        proxies = region_proxies.get(region, [])
        for proxy in proxies:
            proxy_html = f"""
            <div class="ip-item proxy-item">
                <div class="ip-address">{proxy.host}:{proxy.port}</div>
                <div class="ip-meta">
                    <span class="badge badge-latency">å»¶è¿Ÿ {proxy.tested_latency}ms</span>
                    <span class="badge badge-colo">{proxy.type.upper()}</span>
                    <span class="badge badge-score">æ¥æº {proxy.source}</span>
                </div>
            </div>"""
            proxy_items_html.append(proxy_html)
        
        proxy_section = ""
        if proxy_items_html:
            proxy_section = f"""
            <div class="proxy-list">
                <h4>ä»£ç†åˆ—è¡¨ ({len(proxies)})</h4>
                {''.join(proxy_items_html)}
            </div>"""
        
        # åœ°åŒºå¡ç‰‡
        card_html = f"""
        <div class="region-card">
            <div class="region-header">
                <span>{region}</span>
                <span class="region-count">{len(nodes)} èŠ‚ç‚¹</span>
            </div>
            <div class="region-body">
                <div class="ip-list">
                    {''.join(ip_items_html)}
                </div>
                {proxy_section}
                <div class="region-downloads">
                    <a href="ip_{region}.txt" class="region-download-btn btn-primary" download>
                        ğŸ“¥ IPåˆ—è¡¨
                    </a>
                    <a href="proxy_{region}.txt" class="region-download-btn btn-success" download>
                        ğŸ”‘ ä»£ç†åˆ—è¡¨
                    </a>
                </div>
            </div>
        </div>"""
        region_cards_html.append(card_html)
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_proxies = sum(len(proxies) for proxies in region_proxies.values())
    
    # æ›¿æ¢æ¨¡æ¿å˜é‡
    html_content = template.replace('{{GENERATED_TIME}}', datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S UTC'))
    html_content = html_content.replace('{{TOTAL_NODES}}', str(len(all_nodes)))
    html_content = html_content.replace('{{TOTAL_REGIONS}}', str(len(region_results)))
    html_content = html_content.replace('{{TOTAL_PROXIES}}', str(total_proxies))
    html_content = html_content.replace('{{REGION_CARDS}}', '\n'.join(region_cards_html))
    
    # ä¿å­˜HTMLæ–‡ä»¶
    with open(f"{OUTPUT_DIR}/index.html", "w", encoding="utf-8") as f:
        f.write(html_content)
    
    logging.info(f"âœ“ ç”ŸæˆHTMLé¡µé¢: {OUTPUT_DIR}/index.html")


# =========================
# ä¸»æµç¨‹
# =========================

def main():
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    os.makedirs(DATA_DIR, exist_ok=True)
    
    logging.info(f"\n{'#'*60}")
    logging.info(f"# Cloudflare IP ä¼˜é€‰æ‰«æå™¨ V2.0 API Edition")
    logging.info(f"# æ•°æ®æº: Proxifly + ProxyDaily + Tomcat1235 + Hookzof + Proxyscrape")
    logging.info(f"# æ”¯æŒåè®®: HTTPS + SOCKS5 (ä¼˜å…ˆ)")
    logging.info(f"# æ£€æµ‹æ–¹å¼: APIæ™ºèƒ½æ£€æµ‹")
    logging.info(f"# æ¯åœ°åŒºä»£ç†æ•°: {MAX_PROXIES_PER_REGION}")
    logging.info(f"# æ¯åœ°åŒºè¾“å‡ºæ•°: {MAX_OUTPUT_PER_REGION}")
    logging.info(f"{'#'*60}\n")
    
    # æ£€æŸ¥APIé…ç½®
    if not PROXY_CHECK_API_URL:
        logging.warning("âš  æœªé…ç½® PROXY_CHECK_API_URL")
        logging.warning("âš  è¯·åœ¨è„šæœ¬å¼€å¤´è®¾ç½® PROXY_CHECK_API_URL å’Œ PROXY_CHECK_API_TOKEN")
        logging.warning("âš  å°†ç»§ç»­è¿è¡Œä½†ä»£ç†æ£€æµ‹åŠŸèƒ½å°†ä¸å¯ç”¨\n")
    
    # è¿è¡Œå†…éƒ¨æµ‹è¯•
    if not run_internal_tests():
        logging.error("\nâŒ å†…éƒ¨æµ‹è¯•æœªé€šè¿‡,ç¨‹åºé€€å‡º")
        return
    
    logging.info("\n" + "="*60)
    logging.info("å¼€å§‹æ­£å¼æ‰«æ...")
    logging.info("="*60)
    
    # è·å– Cloudflare IP æ®µ
    logging.info("\nè·å– Cloudflare IP èŒƒå›´...")
    cidrs = fetch_cf_ipv4_cidrs()
    
    # ç”Ÿæˆæµ‹è¯• IP æ± 
    total_ips = sum(cfg["sample"] for cfg in REGION_CONFIG.values())
    logging.info(f"ç”Ÿæˆ {total_ips} ä¸ªæµ‹è¯• IP...\n")
    all_test_ips = weighted_random_ips(cidrs, total_ips)
    
    all_results = []
    region_results = {}
    region_proxies = {}  # å­˜å‚¨æ¯ä¸ªåœ°åŒºçš„ä»£ç†
    
    ip_offset = 0
    for region, config in REGION_CONFIG.items():
        sample_size = config["sample"]
        region_ips = all_test_ips[ip_offset:ip_offset + sample_size]
        ip_offset += sample_size
        
        # è·å–è¯¥åœ°åŒºçš„æœ€ä½³ä»£ç†
        proxies = get_proxies(region)
        region_proxies[region] = proxies  # ä¿å­˜ä»£ç†åˆ—è¡¨
        
        # æ‰«æ
        raw = scan_region(region, region_ips, proxies)
        nodes = aggregate_nodes(raw)
        
        region_results[region] = nodes
        all_results.extend(raw)
        
        logging.info(f"{'='*60}")
        logging.info(f"âœ“ {region}: å‘ç° {len(nodes)} ä¸ªæœ‰æ•ˆèŠ‚ç‚¹")
        logging.info(f"{'='*60}\n")
        
        time.sleep(1)
    
    # æ±‡æ€»æ‰€æœ‰èŠ‚ç‚¹
    all_nodes = aggregate_nodes(all_results)
    all_nodes.sort(key=lambda x: x["score"], reverse=True)
    
    logging.info(f"\n{'='*60}")
    logging.info(f"æ€»è®¡å‘ç° {len(all_nodes)} ä¸ªèŠ‚ç‚¹")
    logging.info(f"{'='*60}\n")
    
    # ä¿å­˜æ€»æ–‡ä»¶
    all_lines = [f'{n["ip"]}:{n["port"]}#{n["region"]}-score{n["score"]}\n' for n in all_nodes]
    
    with open(f"{OUTPUT_DIR}/ip_all.txt", "w") as f:
        f.writelines(all_lines)
    
    # æŒ‰åœ°åŒºä¿å­˜IP
    for region, nodes in region_results.items():
        nodes.sort(key=lambda x: x["score"], reverse=True)
        top_nodes = nodes[:MAX_OUTPUT_PER_REGION]
        
        with open(f"{OUTPUT_DIR}/ip_{region}.txt", "w") as f:
            for n in top_nodes:
                f.write(f'{n["ip"]}:{n["port"]}#{region}-score{n["score"]}\n')
        
        logging.info(f"{region}: ä¿å­˜ {len(top_nodes)} ä¸ªèŠ‚ç‚¹")
    
    # ä¿å­˜ä»£ç†åˆ—è¡¨
    save_proxy_list(region_proxies)
    
    # ä¿å­˜ JSON
    with open(f"{OUTPUT_DIR}/ip_candidates.json", "w") as f:
        json.dump({
            "meta": {
                "generated_at": datetime.utcnow().isoformat() + "Z",
                "total_nodes": len(all_nodes),
                "regions": {r: len(nodes) for r, nodes in region_results.items()},
                "version": "2.0-api",
                "data_sources": ["proxifly", "proxydaily", "tomcat1235", "hookzof", "proxyscrape"],
                "protocols": ["https", "socks5"],
                "proxy_check_method": "api",
                "total_proxies": sum(len(proxies) for proxies in region_proxies.values())
            },
            "nodes": all_nodes[:200]
        }, f, indent=2)
    
    # ç”ŸæˆHTMLé¡µé¢
    generate_html(all_nodes, region_results, region_proxies)
    
    # æ‰“å°ç»Ÿè®¡
    print("\n" + "="*60)
    print("ğŸ“Š æ‰«æç»Ÿè®¡")
    print("="*60)
    for region in sorted(region_results.keys()):
        nodes = region_results[region]
        proxies = region_proxies.get(region, [])
        if nodes:
            avg_score = sum(n["score"] for n in nodes) / len(nodes)
            print(f"{region:4s}: {len(nodes):3d} èŠ‚ç‚¹ | {len(proxies):2d} ä»£ç† | å¹³å‡åˆ†æ•°: {avg_score:.3f}")
    
    total_proxies = sum(len(p) for p in region_proxies.values())
    print("="*60)
    print(f"æ€»ä»£ç†æ•°: {total_proxies}")
    print("="*60)
    
    logging.info("\nâœ… æ‰«æå®Œæˆ!")
    logging.info(f"ç»“æœå·²ä¿å­˜åˆ° {OUTPUT_DIR}/ ç›®å½•")
    logging.info(f"  - IPåˆ—è¡¨: ip_all.txt, ip_[REGION].txt")
    logging.info(f"  - ä»£ç†åˆ—è¡¨: proxy_all.txt, proxy_[REGION].txt")
    logging.info(f"  - JSONæ•°æ®: ip_candidates.json")
    logging.info(f"  - HTMLé¡µé¢: index.html")

if __name__ == "__main__":
    main()