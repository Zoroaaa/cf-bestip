# main.pyï¼ˆæ›´æ–°ç‰ˆï¼‰
import subprocess
import random
import ipaddress
import requests
import os
import json
import time
import logging
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime

# å¯¼å…¥æ–°æ¨¡å—
from config import *
from models import ProxyInfo
from data_sources import DataSourceManager
from proxy_tester import ProxyTester
from output_manager import OutputManager
from internal_tester import InternalTester  # æ–°å¢å¯¼å…¥

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[logging.StreamHandler()]
)

# =========================
# å·¥å…·å‡½æ•°ï¼ˆä¿ç•™ï¼‰
# =========================

def fetch_cf_ipv4_cidrs():
    r = requests.get(CF_IPS_V4_URL, timeout=10)
    r.raise_for_status()
    return [x.strip() for x in r.text.splitlines() if x.strip()]

def weighted_random_ips(cidrs, total):
    pools = []
    for c in cidrs:
        net = ipaddress.ip_network(c)
        pools.append((net, net.num_addresses))
    
    total_weight = sum(w for _, w in pools)
    result = []
    
    for net, weight in pools:
        cnt = max(1, int(total * weight / total_weight))
        hosts = list(net.hosts())
        if hosts:
            result.extend(random.sample(hosts, min(cnt, len(hosts))))
    
    random.shuffle(result)
    return result[:total]

def score_ip(latencies):
    if len(latencies) < 2:
        return 0
    
    lat_min = min(latencies)
    lat_max = max(latencies)
    
    s_stability = len(latencies) / len(TRACE_DOMAINS)
    s_consistency = max(0.3, 1 - (lat_max - lat_min) / LATENCY_LIMIT)
    s_latency = 1 / (1 + lat_min / 100)
    
    return round(s_stability * s_consistency * s_latency, 4)

def aggregate_nodes(raw):
    ip_map = defaultdict(list)
    for r in raw:
        ip_map[r["ip"]].append(r)
    
    nodes = []
    for ip, items in ip_map.items():
        latencies = [x["latency"] for x in items]
        score = score_ip(latencies)
        if score <= 0:
            continue
        
        best = min(items, key=lambda x: x["latency"])
        nodes.append({
            "ip": ip,
            "port": random.choice(HTTPS_PORTS),
            "region": best["region"],
            "colo": best["colo"],
            "latencies": latencies,
            "score": score
        })
    
    return nodes

def curl_test_with_proxy(ip, domain, proxy=None):
    """ä½¿ç”¨ä»£ç†æµ‹è¯• Cloudflare IP"""
    try:
        cmd = ["curl", "-k", "-o", "/dev/null", "-s"]
        
        if proxy:
            if proxy.type in ['socks5', 'socks4']:
                cmd.extend(["--socks5", f"{proxy.host}:{proxy.port}"])
            else:
                cmd.extend(["-x", f"http://{proxy.host}:{proxy.port}"])
        
        cmd.extend([
            "-w", "%{time_connect} %{time_appconnect} %{http_code}",
            "--http1.1",
            "--connect-timeout", str(CONNECT_TIMEOUT + 2),
            "--max-time", str(TIMEOUT + 3),
            "--resolve", f"{domain}:443:{ip}",
            f"https://{domain}"
        ])
        
        out = subprocess.check_output(cmd, timeout=TIMEOUT + 5, stderr=subprocess.DEVNULL)
        parts = out.decode().strip().split()
        
        if len(parts) < 3:
            return None
        
        tc, ta, code = parts[0], parts[1], parts[2]
        
        if code in ["000", "0"]:
            return None
        
        latency = int((float(tc) + float(ta)) * 1000)
        
        if latency > LATENCY_LIMIT:
            return None
        
        # è·å– CF-Ray
        hdr_cmd = ["curl", "-k", "-sI"]
        
        if proxy:
            if proxy.type in ['socks5', 'socks4']:
                hdr_cmd.extend(["--socks5", f"{proxy.host}:{proxy.port}"])
            else:
                hdr_cmd.extend(["-x", f"http://{proxy.host}:{proxy.port}"])
        
        hdr_cmd.extend([
            "--connect-timeout", str(CONNECT_TIMEOUT + 2),
            "--max-time", str(TIMEOUT + 3),
            "--resolve", f"{domain}:443:{ip}",
            f"https://{domain}"
        ])
        
        hdr = subprocess.check_output(
            hdr_cmd,
            timeout=TIMEOUT + 3,
            stderr=subprocess.DEVNULL
        ).decode(errors="ignore").lower()
        
        ray = None
        for line in hdr.splitlines():
            if line.startswith("cf-ray"):
                ray = line.split(":")[1].strip()
                break
        
        if not ray:
            return None
        
        colo = ray.split("-")[-1].upper()
        region = COLO_MAP.get(colo, "UNMAPPED")
        
        return {
            "ip": str(ip),
            "domain": domain,
            "colo": colo,
            "region": region,
            "latency": latency,
            "proxy": f"{proxy.host}:{proxy.port}({proxy.type})" if proxy else "direct"
        }
        
    except subprocess.TimeoutExpired:
        return None
    except Exception as e:
        logging.debug(f"æµ‹è¯•å¤±è´¥: {ip} - {e}")
        return None

def test_ip_with_proxy(ip, proxy=None):
    records = []
    for view, domain in TRACE_DOMAINS.items():
        r = curl_test_with_proxy(ip, domain, proxy)
        if r:
            r["view"] = view
            records.append(r)
    return records

def scan_region(region, ips, proxies):
    logging.info(f"\n{'='*60}")
    logging.info(f"å¼€å§‹æ‰«æåœ°åŒº: {region}")
    logging.info(f"{'='*60}")
    
    raw_results = []
    
    if proxies:
        logging.info(f"ä½¿ç”¨ {len(proxies)} ä¸ªä»£ç†è¿›è¡Œæ‰«æ...")
        
        ips_per_proxy = max(1, len(ips) // len(proxies))
        
        for i, proxy in enumerate(proxies):
            proxy_ips = ips[i*ips_per_proxy:(i+1)*ips_per_proxy]
            
            if not proxy_ips:
                continue
            
            proxy_info = f"{proxy.host}:{proxy.port}({proxy.type})"
            logging.info(f"  â†’ é€šè¿‡ä»£ç† {proxy_info} æµ‹è¯• {len(proxy_ips)} ä¸ªIP...")
            
            with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
                futures = [executor.submit(test_ip_with_proxy, ip, proxy) for ip in proxy_ips]
                
                for future in as_completed(futures):
                    try:
                        batch = future.result(timeout=TIMEOUT + 5)
                        if batch:
                            raw_results.extend(batch)
                    except:
                        pass
        
        logging.info(f"  âœ“ ä»£ç†æ‰«ææ”¶é›†: {len(raw_results)} æ¡ç»“æœ")
    
    # åŠ¨æ€è¡¥å……ç­–ç•¥
    expected_results = len(ips) * 0.2
    
    if len(raw_results) < expected_results:
        supplement_count = len(ips) // 2 if raw_results else len(ips)
        logging.info(f"âš  ä»£ç†ç»“æœä¸è¶³({len(raw_results)}/{expected_results:.0f}),ä½¿ç”¨ç›´è¿è¡¥å…… {supplement_count} ä¸ªIP...")
        
        remaining_ips = ips[:supplement_count]
        
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            futures = [executor.submit(test_ip_with_proxy, ip, None) for ip in remaining_ips]
            
            for future in as_completed(futures):
                try:
                    batch = future.result(timeout=TIMEOUT + 5)
                    if batch:
                        raw_results.extend(batch)
                except:
                    pass
        
        logging.info(f"  âœ“ ç›´è¿è¡¥å……æ”¶é›†,å½“å‰æ€»è®¡: {len(raw_results)} æ¡ç»“æœ")
    else:
        logging.info(f"  âœ“ ä»£ç†ç»“æœå……è¶³,è·³è¿‡ç›´è¿è¡¥å……")
    
    logging.info(f"âœ“ {region}: æ€»è®¡æ”¶é›† {len(raw_results)} æ¡æµ‹è¯•ç»“æœ\n")
    return raw_results

# =========================
# ä¸»æµç¨‹
# =========================

def main():
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    os.makedirs(DATA_DIR, exist_ok=True)
    
    logging.info(f"\n{'#'*60}")
    logging.info(f"# Cloudflare IP ä¼˜é€‰æ‰«æå™¨ V3.0")
    logging.info(f"# æ¨¡å—åŒ–æ¶æ„ | æ•°æ®æºåˆ†ç¦» | ç‹¬ç«‹æµ‹è¯•æ¨¡å—")
    logging.info(f"{'#'*60}\n")
    
    # æ£€æŸ¥APIé…ç½®
    if not PROXY_CHECK_API_URL:
        logging.warning("âš  æœªé…ç½® PROXY_CHECK_API_URL")
        logging.warning("âš  è¯·åœ¨ config.py ä¸­è®¾ç½® PROXY_CHECK_API_URL å’Œ PROXY_CHECK_API_TOKEN")
        logging.warning("âš  å°†ç»§ç»­è¿è¡Œä½†ä»£ç†æ£€æµ‹åŠŸèƒ½å°†ä¸å¯ç”¨\n")
    
    # è¿è¡Œå†…éƒ¨æµ‹è¯•
    internal_tester = InternalTester()
    if not internal_tester.run_all_tests():
        logging.error("\nâŒâŒ å†…éƒ¨æµ‹è¯•æœªé€šè¿‡,ç¨‹åºé€€å‡º")
        return
    
    logging.info("\n" + "="*60)
    logging.info("å¼€å§‹æ­£å¼æ‰«æ...")
    logging.info("="*60)
    
    # åˆå§‹åŒ–ç®¡ç†å™¨
    data_source_manager = DataSourceManager()
    proxy_tester = ProxyTester()
    output_manager = OutputManager()
    
    # è·å– Cloudflare IP æ®µ
    logging.info("\nè·å– Cloudflare IP èŒƒå›´...")
    cidrs = fetch_cf_ipv4_cidrs()
    
    # ç”Ÿæˆæµ‹è¯• IP æ± 
    total_ips = sum(cfg["sample"] for cfg in REGION_CONFIG.values())
    logging.info(f"ç”Ÿæˆ {total_ips} ä¸ªæµ‹è¯• IP...\n")
    all_test_ips = weighted_random_ips(cidrs, total_ips)
    
    all_results = []
    region_results = {}
    region_proxies = {}  # å­˜å‚¨æ¯ä¸ªåœ°åŒºçš„ä»£ç†
    
    ip_offset = 0
    for region, config in REGION_CONFIG.items():
        sample_size = config["sample"]
        region_ips = all_test_ips[ip_offset:ip_offset + sample_size]
        ip_offset += sample_size
        
        # è·å–è¯¥åœ°åŒºçš„æœ€ä½³ä»£ç†
        raw_proxies = data_source_manager.fetch_all_proxies(region)
        proxies = proxy_tester.test_proxies(raw_proxies, region)
        region_proxies[region] = proxies  # ä¿å­˜ä»£ç†åˆ—è¡¨
        
        # æ‰«æ
        raw = scan_region(region, region_ips, proxies)
        nodes = aggregate_nodes(raw)
        
        region_results[region] = nodes
        all_results.extend(raw)
        
        logging.info(f"{'='*60}")
        logging.info(f"âœ“ {region}: å‘ç° {len(nodes)} ä¸ªæœ‰æ•ˆèŠ‚ç‚¹")
        logging.info(f"{'='*60}\n")
        
        time.sleep(1)
    
    # æ±‡æ€»æ‰€æœ‰èŠ‚ç‚¹
    all_nodes = aggregate_nodes(all_results)
    all_nodes.sort(key=lambda x: x["score"], reverse=True)
    
    logging.info(f"\n{'='*60}")
    logging.info(f"æ€»è®¡å‘ç° {len(all_nodes)} ä¸ªèŠ‚ç‚¹")
    logging.info(f"{'='*60}\n")
    
    # ä¿å­˜ç»“æœ
    output_manager.save_results(all_nodes, region_results, region_proxies)
    
    # æ‰“å°ç»Ÿè®¡
    print("\n" + "="*60)
    print("ğŸ“ŠğŸ“Š æ‰«æç»Ÿè®¡")
    print("="*60)
    for region in sorted(region_results.keys()):
        nodes = region_results[region]
        proxies = region_proxies.get(region, [])
        if nodes:
            avg_score = sum(n["score"] for n in nodes) / len(nodes)
            print(f"{region:4s}: {len(nodes):3d} èŠ‚ç‚¹ | {len(proxies):2d} ä»£ç† | å¹³å‡åˆ†æ•°: {avg_score:.3f}")
    
    total_proxies = sum(len(p) for p in region_proxies.values())
    print("="*60)
    print(f"æ€»ä»£ç†æ•°: {total_proxies}")
    print("="*60)
    
    logging.info("\nâœ… æ‰«æå®Œæˆ!")
    logging.info(f"ç»“æœå·²ä¿å­˜åˆ° {OUTPUT_DIR}/ ç›®å½•")
    logging.info(f"  - IPåˆ—è¡¨: ip_all.txt, ip_[REGION].txt")
    logging.info(f"  - ä»£ç†åˆ—è¡¨: proxy_all.txt, proxy_[REGION].txt")
    logging.info(f"  - JSONæ•°æ®: ip_candidates.json")
    logging.info(f"  - HTMLé¡µé¢: index.html")

if __name__ == "__main__":
    main()