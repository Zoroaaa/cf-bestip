import subprocess
import random
import ipaddress
import requests
import os
import json
import time
import logging
import socket
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime, timedelta
import statistics

# =========================
# é…ç½®æ—¥å¿—
# =========================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[logging.StreamHandler()]
)

# =========================
# åŸºç¡€å‚æ•°
# =========================

CF_IPS_V4_URL = "https://www.cloudflare.com/ips-v4"

TRACE_DOMAINS = {
    "v0": "sptest.ittool.pp.ua",
    "v1": "sptest1.ittool.pp.ua",
    "v2": "sptest2.ittool.pp.ua",
}

SAMPLE_SIZE = 820
TIMEOUT = 6
CONNECT_TIMEOUT = 3
MAX_WORKERS = 24
LATENCY_LIMIT = 500

OUTPUT_DIR = "public"
DATA_DIR = "public/data"
PROXY_CACHE_DIR = "proxy_cache"

HTTPS_PORTS = [443, 8443, 2053, 2083, 2087, 2096]

# ç›®æ ‡åœ°åŒºé…ç½®
REGION_CONFIG = {
    "HK": {"codes": ["HK"], "sample": 100},
    "SG": {"codes": ["SG"], "sample": 100},
    "JP": {"codes": ["JP"], "sample": 100},
    "KR": {"codes": ["KR"], "sample": 80},
    "TW": {"codes": ["TW"], "sample": 80},
    "US": {"codes": ["US"], "sample": 120},
    "DE": {"codes": ["DE"], "sample": 60},
    "UK": {"codes": ["GB"], "sample": 60},
    "AU": {"codes": ["AU"], "sample": 60},
    "CA": {"codes": ["CA"], "sample": 60},
}

MAX_OUTPUT_PER_REGION = 16
GOOD_SCORE_THRESHOLD = 0.75
MAX_PROXIES_PER_REGION = 5  # æ¯ä¸ªåœ°åŒºé€‰å‡ºæœ€ä½³çš„5ä¸ªä»£ç†

# ä»£ç†æµ‹è¯•é…ç½®
PROXY_TEST_TIMEOUT = 5
PROXY_QUICK_TEST_URL = "http://www.gstatic.com/generate_204"
PROXY_MAX_LATENCY = 1000
ALLOW_HTTP_ONLY_PROXY = True  # æ˜¯å¦å…è®¸ä½¿ç”¨ä¸æ”¯æŒ HTTPS çš„ä»£ç†ï¼ˆä¼šé™ä½æˆåŠŸç‡ä½†å¢åŠ ä»£ç†å¯ç”¨æ€§ï¼‰
PROXY_HTTP_LATENCY_PENALTY = 100  # HTTP-only ä»£ç†çš„å»¶è¿Ÿæƒ©ç½šï¼ˆmsï¼‰- é™ä½æƒ©ç½šå€¼ï¼Œç»™æ›´å¤šæœºä¼š

# =========================
# COLO â†’ Region
# =========================

COLO_MAP = {
    "HKG": "HK", "SIN": "SG", "NRT": "JP", "KIX": "JP",
    "ICN": "KR", "TPE": "TW",
    "SYD": "AU", "MEL": "AU",
    "LAX": "US", "SJC": "US", "SFO": "US",
    "SEA": "US", "ORD": "US", "DFW": "US",
    "ATL": "US", "IAD": "US", "EWR": "US",
    "JFK": "US", "BOS": "US", "MIA": "US",
    "PHX": "US", "DEN": "US", "IAH": "US",
    "FRA": "DE", "MUC": "DE", "AMS": "DE",
    "LHR": "UK", "LGW": "UK", "MAN": "UK",
    "YYZ": "CA", "YVR": "CA",
}

# =========================
# Proxifly ä»£ç†åˆ—è¡¨åŸºç¡€ URL
# =========================
PROXIFLY_BASE_URL = "https://cdn.jsdelivr.net/gh/proxifly/free-proxy-list@main/proxies/countries/{}/data.txt"

# åœ°åŒºä»£ç æ˜ å°„ï¼ˆREGION_CONFIG key -> Proxifly country codeï¼‰
REGION_TO_COUNTRY_CODE = {
    "HK": "HK",
    "SG": "SG",
    "JP": "JP",
    "KR": "KR",
    "TW": "TW",
    "US": "US",
    "DE": "DE",
    "UK": "GB",
    "AU": "AU",
    "CA": "CA",
}

# =========================
# ä» Proxifly è·å–ä»£ç†åˆ—è¡¨
# =========================

def fetch_proxifly_proxies(region):
    """
    ä» Proxifly è·å–æŒ‡å®šåœ°åŒºçš„ä»£ç†åˆ—è¡¨
    è¿”å›æ ¼å¼: [{"host": "1.2.3.4", "port": 8080, "type": "http"}, ...]
    """
    country_code = REGION_TO_COUNTRY_CODE.get(region)
    if not country_code:
        logging.warning(f"{region} æ— å¯¹åº”çš„ Proxifly å›½å®¶ä»£ç ")
        return []
    
    url = PROXIFLY_BASE_URL.format(country_code)
    
    try:
        logging.info(f"æ­£åœ¨ä» Proxifly è·å– {region} çš„ä»£ç†åˆ—è¡¨...")
        response = requests.get(url, timeout=15)
        response.raise_for_status()
        
        proxies = []
        lines = response.text.strip().split('\n')
        
        for line in lines:
            line = line.strip()
            if not line or line.startswith('#'):
                continue
            
            try:
                # æ ¼å¼: http://IP:PORT æˆ– socks5://IP:PORT
                if line.startswith('http://'):
                    proxy_type = 'http'
                    line = line.replace('http://', '')
                elif line.startswith('socks5://'):
                    proxy_type = 'socks5'
                    line = line.replace('socks5://', '')
                elif line.startswith('socks4://'):
                    proxy_type = 'socks4'
                    line = line.replace('socks4://', '')
                else:
                    # æ²¡æœ‰åè®®å‰ç¼€ï¼Œé»˜è®¤ä¸º http
                    proxy_type = 'http'
                
                # è§£æ IP:PORT
                parts = line.split(':')
                if len(parts) >= 2:
                    host = parts[0].strip()
                    port = int(parts[1].strip())
                    
                    # éªŒè¯ IP æ ¼å¼
                    ipaddress.ip_address(host)
                    
                    proxies.append({
                        "host": host,
                        "port": port,
                        "type": proxy_type
                    })
            except (ValueError, ipaddress.AddressValueError, IndexError):
                logging.debug(f"è·³è¿‡æ— æ•ˆä»£ç†è¡Œ: {line}")
                continue
        
        logging.info(f"âœ“ {region}: è·å–åˆ° {len(proxies)} ä¸ªä»£ç†")
        return proxies
    
    except requests.RequestException as e:
        logging.error(f"âœ— {region}: è·å–ä»£ç†åˆ—è¡¨å¤±è´¥ - {e}")
        return []

# =========================
# ä»£ç†æµ‹è¯•å‡½æ•°
# =========================

def test_proxy_latency(proxy):
    """
    æµ‹è¯•ä»£ç†çš„è¿é€šæ€§å’Œå»¶è¿Ÿï¼ˆæ”¹è¿›ç‰ˆï¼šä¸¥æ ¼è¦æ±‚HTTPSæµ‹è¯•è¿”å›æˆåŠŸçŠ¶æ€ç ï¼‰
    è¿”å›: {"success": True, "latency": 123, "https_ok": True/False}
    """
    host = proxy["host"]
    port = proxy["port"]
    proxy_type = proxy.get("type", "http")
    
    start = time.time()
    
    try:
        # å…ˆæµ‹è¯• HTTPï¼ˆåŸºç¡€è¿é€šæ€§ï¼‰
        cmd = ["curl", "-s", "-o", "/dev/null", "-w", "%{http_code}"]
        
        if proxy_type in ["socks5", "socks4"]:
            cmd.extend(["--socks5", f"{host}:{port}"])
        else:
            cmd.extend(["-x", f"http://{host}:{port}"])
        
        cmd.extend([
            "--connect-timeout", str(PROXY_TEST_TIMEOUT),
            "--max-time", str(PROXY_TEST_TIMEOUT),
            PROXY_QUICK_TEST_URL
        ])
        
        result = subprocess.run(
            cmd,
            capture_output=True,
            timeout=PROXY_TEST_TIMEOUT + 2
        )
        
        latency = int((time.time() - start) * 1000)
        
        if result.returncode != 0:
            return {"success": False, "latency": 999999, "https_ok": False}
        
        http_code = result.stdout.decode().strip()
        if http_code not in ["204", "200", "301", "302"]:
            return {"success": False, "latency": 999999, "https_ok": False}
        
        # æµ‹è¯• HTTPS æ”¯æŒï¼ˆä¸¥æ ¼è¦æ±‚æˆåŠŸçŠ¶æ€ç ï¼‰
        https_start = time.time()
        https_cmd = ["curl", "-k", "-s", "-o", "/dev/null", "-w", "%{http_code}"]
        
        if proxy_type in ["socks5", "socks4"]:
            https_cmd.extend(["--socks5", f"{host}:{port}"])
        else:
            https_cmd.extend(["-x", f"http://{host}:{port}"])
        
        https_cmd.extend([
            "--connect-timeout", str(PROXY_TEST_TIMEOUT),
            "--max-time", str(PROXY_TEST_TIMEOUT),
            "https://www.cloudflare.com/cdn-cgi/trace"
        ])
        
        https_result = subprocess.run(
            https_cmd,
            capture_output=True,
            timeout=PROXY_TEST_TIMEOUT + 2
        )
        
        https_latency = int((time.time() - https_start) * 1000)
        
        # ä¸¥æ ¼æ£€æŸ¥HTTPSçŠ¶æ€ç 
        https_http_code = https_result.stdout.decode().strip()
        https_ok = https_http_code in ["200", "204", "301", "302"]
        
        # å¦‚æœ HTTPS æµ‹è¯•æˆåŠŸï¼Œä½¿ç”¨ HTTPS å»¶è¿Ÿï¼›å¦åˆ™ä½¿ç”¨ HTTP å»¶è¿Ÿ
        final_latency = https_latency if https_ok else latency
        
        return {
            "success": True, 
            "latency": final_latency,
            "https_ok": https_ok
        }
    
    except Exception as e:
        logging.debug(f"ä»£ç† {host}:{port} æµ‹è¯•å¤±è´¥: {e}")
        return {"success": False, "latency": 999999, "https_ok": False}

# =========================
# è·å–è¯¥åœ°åŒºçš„æœ€ä½³ä»£ç†ï¼ˆtop 5ï¼‰
# =========================

def get_proxies(region):
    """
    è·å–æŒ‡å®šåœ°åŒºçš„æœ€ä½³ä»£ç†ï¼ˆä»…åŸºç¡€è¿é€šæ€§éªŒè¯ï¼‰
    """
    # ä» Proxifly è·å–ä»£ç†åˆ—è¡¨
    proxies = fetch_proxifly_proxies(region)
    
    if not proxies:
        logging.warning(f"{region} æ— å¯ç”¨ä»£ç†")
        return []
    
    # é™åˆ¶æµ‹è¯•æ•°é‡
    test_proxies = proxies[:50] if len(proxies) > 50 else proxies
    
    logging.info(f"{region} æµ‹è¯• {len(test_proxies)} ä¸ªä»£ç†çš„åŸºç¡€è¿é€šæ€§...")
    
    # åŸºç¡€è¿é€šæ€§æµ‹è¯•ï¼ˆå¿«é€Ÿç­›é€‰ï¼‰
    candidate_proxies = []
    
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_proxy = {executor.submit(test_proxy_latency, p): p for p in test_proxies}
        
        for future in as_completed(future_to_proxy):
            proxy = future_to_proxy[future]
            try:
                test_result = future.result()
                if test_result["success"] and test_result["latency"] < PROXY_MAX_LATENCY:
                    candidate_proxies.append({
                        "host": proxy["host"],
                        "port": proxy["port"],
                        "type": proxy["type"],
                        "basic_latency": test_result["latency"],
                        "https_ok": test_result["https_ok"]
                    })
            except Exception as e:
                logging.debug(f"ä»£ç†æµ‹è¯•å¼‚å¸¸: {e}")
    
    if not candidate_proxies:
        logging.warning(f"âš  {region} æ— å¯ç”¨ä»£ç†ï¼Œå°†å®Œå…¨ä½¿ç”¨ç›´è¿")
        return []
    
    logging.info(f"  âœ“ é€šè¿‡: {len(candidate_proxies)} ä¸ªä»£ç†")
    
    # åˆ†ç¦» HTTPS å’Œ HTTP-only ä»£ç†
    https_proxies = [p for p in candidate_proxies if p["https_ok"]]
    http_proxies = [p for p in candidate_proxies if not p["https_ok"]]
    
    # æŒ‰ basic_latency æ’åº
    https_proxies.sort(key=lambda x: x["basic_latency"])
    http_proxies.sort(key=lambda x: x["basic_latency"])
    
    # å…ˆå– HTTPS ä»£ç†çš„å‰ MAX_PROXIES_PER_REGION ä¸ª
    best_proxies = https_proxies[:MAX_PROXIES_PER_REGION]
    
    # å¦‚æœä¸è¶³ï¼Œè¡¥ HTTP-only ä»£ç†
    remaining = MAX_PROXIES_PER_REGION - len(best_proxies)
    if remaining > 0:
        best_proxies.extend(http_proxies[:remaining])
    
    logging.info(f"âœ“ {region} æœ€ç»ˆé€‰å‡º {len(best_proxies)} ä¸ªå¯ç”¨ä»£ç†:")
    for i, p in enumerate(best_proxies, 1):
        https_mark = "[HTTPSâœ“]" if p.get("https_ok") else "[HTTP-tunnel]"
        logging.info(f"  {i}. {p['host']}:{p['port']} ({p['type']}) - å»¶è¿Ÿ:{p['basic_latency']}ms {https_mark}")
    
    return best_proxies

# =========================
# IP æµ‹è¯•å‡½æ•°
# =========================

def curl_test_with_proxy(ip, domain, proxy=None):
    """ä½¿ç”¨ä»£ç†æµ‹è¯• Cloudflare IPï¼ˆæ”¹è¿›ç‰ˆï¼šå¢åŠ å®¹é”™å’Œè°ƒè¯•ï¼‰"""
    try:
        cmd = ["curl", "-k", "-o", "/dev/null", "-s"]
        
        # æ·»åŠ ä»£ç†
        if proxy:
            proxy_type = proxy.get('type', 'http')
            if proxy_type in ['socks5', 'socks4']:
                cmd.extend(["--socks5", f"{proxy['host']}:{proxy['port']}"])
            else:
                # HTTP/HTTPS ä»£ç†
                cmd.extend(["-x", f"http://{proxy['host']}:{proxy['port']}"])
        
        cmd.extend([
            "-w", "%{time_connect} %{time_appconnect} %{http_code}",
            "--http1.1",
            "--connect-timeout", str(CONNECT_TIMEOUT + 2),  # å¢åŠ è¶…æ—¶
            "--max-time", str(TIMEOUT + 3),
            "--resolve", f"{domain}:443:{ip}",
            f"https://{domain}"
        ])
        
        out = subprocess.check_output(cmd, timeout=TIMEOUT + 5, stderr=subprocess.DEVNULL)
        parts = out.decode().strip().split()
        
        if len(parts) < 3:
            return None
        
        tc, ta, code = parts[0], parts[1], parts[2]
        
        # æ”¾å®½æ¡ä»¶ï¼šæ¥å—æ›´å¤š HTTP çŠ¶æ€ç 
        if code in ["000", "0"]:
            return None
        
        latency = int((float(tc) + float(ta)) * 1000)
        
        if latency > LATENCY_LIMIT:
            return None
        
        # è·å– CF-Rayï¼ˆå¢åŠ é‡è¯•é€»è¾‘ï¼‰
        hdr_cmd = ["curl", "-k", "-sI"]
        
        if proxy:
            proxy_type = proxy.get('type', 'http')
            if proxy_type in ['socks5', 'socks4']:
                hdr_cmd.extend(["--socks5", f"{proxy['host']}:{proxy['port']}"])
            else:
                hdr_cmd.extend(["-x", f"http://{proxy['host']}:{proxy['port']}"])
        
        hdr_cmd.extend([
            "--connect-timeout", str(CONNECT_TIMEOUT + 2),
            "--max-time", str(TIMEOUT + 3),
            "--resolve", f"{domain}:443:{ip}",
            f"https://{domain}"
        ])
        
        hdr = subprocess.check_output(
            hdr_cmd,
            timeout=TIMEOUT + 3,
            stderr=subprocess.DEVNULL
        ).decode(errors="ignore").lower()
        
        ray = None
        for line in hdr.splitlines():
            if line.startswith("cf-ray"):
                ray = line.split(":")[1].strip()
                break
        
        if not ray:
            # æ²¡æœ‰ CF-Ray ä¹Ÿè®°å½•ç»“æœï¼ˆå¯èƒ½æ˜¯ä¸­è½¬èŠ‚ç‚¹ï¼‰
            logging.debug(f"IP {ip} æ—  CF-Rayï¼Œå¯èƒ½ä¸æ˜¯ Cloudflare èŠ‚ç‚¹")
            return None
        
        colo = ray.split("-")[-1].upper()
        region = COLO_MAP.get(colo, "UNMAPPED")
        
        return {
            "ip": str(ip),
            "domain": domain,
            "colo": colo,
            "region": region,
            "latency": latency,
            "proxy": f"{proxy['host']}:{proxy['port']}" if proxy else "direct"
        }
    
    except subprocess.TimeoutExpired:
        logging.debug(f"æµ‹è¯•è¶…æ—¶: {ip} via {proxy['host'] if proxy else 'direct'}")
        return None
    except Exception as e:
        logging.debug(f"æµ‹è¯•å¤±è´¥: {ip} - {e}")
        return None

def test_ip_with_proxy(ip, proxy=None):
    records = []
    for view, domain in TRACE_DOMAINS.items():
        r = curl_test_with_proxy(ip, domain, proxy)
        if r:
            r["view"] = view
            records.append(r)
    return records

# =========================
# å·¥å…·å‡½æ•°
# =========================

def fetch_cf_ipv4_cidrs():
    r = requests.get(CF_IPS_V4_URL, timeout=10)
    r.raise_for_status()
    return [x.strip() for x in r.text.splitlines() if x.strip()]

def weighted_random_ips(cidrs, total):
    pools = []
    for c in cidrs:
        net = ipaddress.ip_network(c)
        pools.append((net, net.num_addresses))
    
    total_weight = sum(w for _, w in pools)
    result = []
    
    for net, weight in pools:
        cnt = max(1, int(total * weight / total_weight))
        hosts = list(net.hosts())
        if hosts:
            result.extend(random.sample(hosts, min(cnt, len(hosts))))
    
    random.shuffle(result)
    return result[:total]

def score_ip(latencies):
    if len(latencies) < 2:
        return 0
    
    # ç¨³å®šæ€§ï¼šæˆåŠŸæ¯”ä¾‹ï¼ˆä¸å˜ï¼‰
    s_stability = len(latencies) / len(TRACE_DOMAINS)
    
    # ä¸€è‡´æ€§ï¼šç”¨æ ‡å‡†å·®ä»£æ›¿ç®€å•max-minï¼Œæ›´ç»Ÿè®¡å­¦æ„ä¹‰
    lat_mean = statistics.mean(latencies)
    lat_stdev = statistics.stdev(latencies) if len(latencies) > 1 else 0
    s_consistency = max(0.3, 1 - (lat_stdev / (LATENCY_LIMIT / 2)))  # æ ‡å‡†å·®é˜ˆå€¼è°ƒæ•´ä¸ºLATENCY_LIMIT/2
    
    # å»¶è¿Ÿï¼šç”¨ä¸­ä½æ•°ä»£æ›¿æœ€å°ï¼ˆæ›´æŠ—å¼‚å¸¸ï¼‰ï¼Œå¹¶ç”¨æŒ‡æ•°è¡°å‡ï¼ˆä½å»¶è¿Ÿæ›´æ•æ„Ÿï¼‰
    lat_median = statistics.median(latencies)
    s_latency = 1 / (1 + (lat_median / 100)**1.2)  # æŒ‡æ•°1.2è®©ä½å»¶è¿Ÿæ›´çªå‡º
    
    # æ€»åˆ†ï¼šåŠ æƒæ±‚å’Œ
    score = 0.4 * s_stability + 0.3 * s_consistency + 0.3 * s_latency
    return round(score, 4)

def aggregate_nodes(raw):
    ip_map = defaultdict(list)
    for r in raw:
        ip_map[r["ip"]].append(r)
    
    nodes = []
    for ip, items in ip_map.items():
        latencies = [x["latency"] for x in items]
        score = score_ip(latencies)
        if score <= 0:
            continue
        
        best = min(items, key=lambda x: x["latency"])
        nodes.append({
            "ip": ip,
            "port": random.choice(HTTPS_PORTS),
            "region": best["region"],
            "colo": best["colo"],
            "latencies": latencies,
            "score": score
        })
    
    return nodes

# =========================
# å†å²IPç®¡ç†å‡½æ•°
# =========================

HISTORICAL_FILE = f"{OUTPUT_DIR}/historical_ips.json"
MAX_HISTORICAL_PER_REGION = 200  # æ¯ä¸ªåœ°åŒºå†å²IPä¸Šé™
FAIL_COUNT_THRESHOLD = 3  # è¿ç»­å¤±è´¥é˜ˆå€¼
VALIDATION_DAYS_THRESHOLD = 7  # å¦‚æœè¶…è¿‡7å¤©æœªæµ‹ï¼Œå¼ºåˆ¶éªŒè¯
VALIDATION_SAMPLE_LIMIT = 50  # æ¯ä¸ªåœ°åŒºæœ€å¤šéªŒè¯50ä¸ªæ—§IP

def load_historical_ips():
    if os.path.exists(HISTORICAL_FILE):
        with open(HISTORICAL_FILE, "r") as f:
            return json.load(f)
    else:
        return {region: [] for region in REGION_CONFIG.keys()}

def save_historical_ips(historical):
    with open(HISTORICAL_FILE, "w") as f:
        json.dump(historical, f, indent=2)

def merge_new_to_historical(historical, new_nodes, current_time, scan_source):
    for node in new_nodes:
        region = node["region"]
        # åªå¤„ç†å·²çŸ¥åœ°åŒº
        if region == "UNMAPPED" or region not in historical:
            skipped += 1
            continue
        ip_key = node["ip"]
        existing = next((item for item in historical[region] if item["ip"] == ip_key), None)
        if existing:
            existing["score"] = node["score"]
            existing["last_tested"] = current_time
            existing["fail_count"] = 0
            existing["source"] = scan_source
            existing["port"] = node["port"]
        else:
            historical[region].append({
                "ip": ip_key,
                "port": node["port"],
                "score": node["score"],
                "last_tested": current_time,
                "fail_count": 0,
                "source": scan_source
            })
    # é™åˆ¶å†å²å¤§å°
    for region in historical:
        historical[region] = sorted(historical[region], key=lambda x: x["score"], reverse=True)[:MAX_HISTORICAL_PER_REGION]

def validate_historical_ips(historical, proxies):
    current_time = datetime.utcnow().isoformat() + "Z"
    current_dt = datetime.fromisoformat(current_time[:-1])  # ç§»é™¤Z
    for region, ips in historical.items():
        to_validate = []
        for ip_entry in ips:
            last_tested_dt = datetime.fromisoformat(ip_entry["last_tested"][:-1]) if "last_tested" in ip_entry else datetime.min
            if (current_dt - last_tested_dt).days > VALIDATION_DAYS_THRESHOLD:
                to_validate.append(ip_entry)
        # é™é‡éªŒè¯
        to_validate = to_validate[:VALIDATION_SAMPLE_LIMIT]
        if not to_validate:
            continue
        logging.info(f"éªŒè¯ {region} çš„ {len(to_validate)} ä¸ªå†å²IP...")
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            futures = []
            for ip_entry in to_validate:
                proxy = random.choice(proxies) if proxies else None
                futures.append(executor.submit(test_ip_with_proxy, ip_entry["ip"], proxy))
            for future, ip_entry in zip(as_completed(futures), to_validate):
                try:
                    records = future.result()
                    if records:
                        latencies = [r["latency"] for r in records]
                        new_score = score_ip(latencies)
                        if new_score > 0:
                            ip_entry["score"] = new_score
                            ip_entry["last_tested"] = current_time
                            ip_entry["fail_count"] = 0
                        else:
                            ip_entry["fail_count"] += 1
                    else:
                        ip_entry["fail_count"] += 1
                except Exception as e:
                    logging.debug(f"éªŒè¯å¤±è´¥: {ip_entry['ip']} - {e}")
                    ip_entry["fail_count"] += 1
        # ç§»é™¤å¤±æ•ˆIP
        historical[region] = [ip for ip in ips if ip["fail_count"] < FAIL_COUNT_THRESHOLD]

# =========================
# åˆ†åœ°åŒºæ‰«æ
# =========================

def scan_region(region, ips, proxies):
    logging.info(f"\n{'='*60}")
    logging.info(f"å¼€å§‹æ‰«æåœ°åŒº: {region}")
    logging.info(f"{'='*60}")
    
    raw_results = []
    
    if proxies:
        logging.info(f"ä½¿ç”¨ {len(proxies)} ä¸ªä»£ç†è¿›è¡Œæ‰«æ...")
        
        ips_per_proxy = max(1, len(ips) // len(proxies))
        
        for i, proxy in enumerate(proxies):
            proxy_ips = ips[i*ips_per_proxy:(i+1)*ips_per_proxy]
            
            if not proxy_ips:
                continue
            
            proxy_info = f"{proxy['host']}:{proxy['port']}"
            logging.info(f"  â†’ é€šè¿‡ä»£ç† {proxy_info} æµ‹è¯• {len(proxy_ips)} ä¸ªIP...")
            
            with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
                futures = [executor.submit(test_ip_with_proxy, ip, proxy) for ip in proxy_ips]
                
                for future in as_completed(futures):
                    try:
                        batch = future.result(timeout=TIMEOUT + 5)
                        if batch:
                            raw_results.extend(batch)
                    except:
                        pass
        
        logging.info(f"  âœ“ ä»£ç†æ‰«ææ”¶é›†: {len(raw_results)} æ¡ç»“æœ")
    
    # åŠ¨æ€è°ƒæ•´è¡¥å……ç­–ç•¥ï¼šå¦‚æœä»£ç†ç»“æœå¤ªå°‘ï¼Œå¢åŠ ç›´è¿æ¯”ä¾‹
    expected_results = len(ips) * 0.2  # æœŸæœ›è‡³å°‘ 20% çš„æˆåŠŸç‡
    
    if len(raw_results) < expected_results:
        supplement_count = len(ips) // 2 if raw_results else len(ips)
        logging.info(f"âš  ä»£ç†ç»“æœä¸è¶³ï¼ˆ{len(raw_results)}/{expected_results:.0f}ï¼‰ï¼Œä½¿ç”¨ç›´è¿è¡¥å……æ‰«æ {supplement_count} ä¸ªIP...")
        
        remaining_ips = ips[:supplement_count]
        
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            futures = [executor.submit(test_ip_with_proxy, ip, None) for ip in remaining_ips]
            
            for future in as_completed(futures):
                try:
                    batch = future.result(timeout=TIMEOUT + 5)
                    if batch:
                        raw_results.extend(batch)
                except:
                    pass
        
        logging.info(f"  âœ“ ç›´è¿è¡¥å……æ”¶é›†ï¼Œå½“å‰æ€»è®¡: {len(raw_results)} æ¡ç»“æœ")
    else:
        logging.info(f"  âœ“ ä»£ç†ç»“æœå……è¶³ï¼Œè·³è¿‡ç›´è¿è¡¥å……")
    
    logging.info(f"âœ“ {region}: æ€»è®¡æ”¶é›† {len(raw_results)} æ¡æµ‹è¯•ç»“æœ\n")
    return raw_results

# =========================
# ä¸»æµç¨‹
# =========================

def main():
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    os.makedirs(DATA_DIR, exist_ok=True)
    
    logging.info(f"\n{'#'*60}")
    logging.info(f"# Cloudflare IP ä¼˜é€‰æ‰«æå™¨ (Proxiflyç‰ˆ)")
    logging.info(f"# ä»£ç†æ¥æº: proxifly/free-proxy-list")
    logging.info(f"# æ¯ä¸ªåœ°åŒºé€‰å‡ºå»¶è¿Ÿæœ€ä½çš„ {MAX_PROXIES_PER_REGION} ä¸ªä»£ç†")
    logging.info(f"# æ¯ä¸ªåœ°åŒºè¾“å‡º top {MAX_OUTPUT_PER_REGION} ä¸ªä¼˜é€‰ IP")
    logging.info(f"{'#'*60}\n")
    
    # åŠ è½½å†å²IP
    historical = load_historical_ips()
    
    # è·å– Cloudflare IP æ®µ
    logging.info("è·å– Cloudflare IP èŒƒå›´...")
    cidrs = fetch_cf_ipv4_cidrs()
    
    # ç”Ÿæˆæµ‹è¯• IP æ± 
    total_ips = sum(cfg["sample"] for cfg in REGION_CONFIG.values())
    logging.info(f"ç”Ÿæˆ {total_ips} ä¸ªæµ‹è¯• IP...\n")
    all_test_ips = weighted_random_ips(cidrs, total_ips)
    
    all_results = []
    region_results = {}
    
    current_time = datetime.utcnow().isoformat() + "Z"
    scan_source = f"scan_{datetime.utcnow().strftime('%Y%m%d')}"
    
    ip_offset = 0
    for region, config in REGION_CONFIG.items():
        sample_size = config["sample"]
        region_ips = all_test_ips[ip_offset:ip_offset + sample_size]
        ip_offset += sample_size
        
        # è·å–è¯¥åœ°åŒºçš„æœ€ä½³ä»£ç†ï¼ˆtop 5ï¼‰
        proxies = get_proxies(region)
        
        # æ‰«æ
        raw = scan_region(region, region_ips, proxies)
        nodes = aggregate_nodes(raw)
        
        # åˆå¹¶æ–°èŠ‚ç‚¹åˆ°å†å²
        merge_new_to_historical(historical, nodes, current_time, scan_source)
        
        # éªŒè¯å†å²IP
        validate_historical_ips(historical, proxies)
        
        # ä»å†å²ä¸­è·å–å½“å‰åœ°åŒºèŠ‚ç‚¹ï¼ˆè¿‡æ»¤æœ‰æ•ˆï¼‰
        region_nodes = [n for n in historical[region] if n["score"] > GOOD_SCORE_THRESHOLD]
        region_nodes.sort(key=lambda x: x["score"], reverse=True)
        region_results[region] = region_nodes
        
        all_results.extend(raw)
        
        logging.info(f"{'='*60}")
        logging.info(f"âœ“ {region}: å†å²ä¸­æœ‰æ•ˆèŠ‚ç‚¹ {len(region_nodes)} ä¸ª")
        logging.info(f"{'='*60}\n")
        
        time.sleep(1)
    
    # ä¿å­˜æ›´æ–°åçš„å†å²
    save_historical_ips(historical)
    
    # æ±‡æ€»æ‰€æœ‰èŠ‚ç‚¹
    all_nodes = []
    for nodes in historical.values():
        all_nodes.extend(nodes)
    all_nodes.sort(key=lambda x: x["score"], reverse=True)
    
    logging.info(f"\n{'='*60}")
    logging.info(f"æ€»è®¡å†å²æœ‰æ•ˆèŠ‚ç‚¹ {len(all_nodes)} ä¸ª")
    logging.info(f"{'='*60}\n")
    
    # ä¿å­˜æ€»æ–‡ä»¶
    all_lines = [f'{n["ip"]}:{n["port"]}#{n["region"]}-score{n["score"]}\n' for n in all_nodes]
    
    with open(f"{OUTPUT_DIR}/ip_all.txt", "w") as f:
        f.writelines(all_lines)
   
    # æŒ‰åœ°åŒºä¿å­˜ï¼ˆtop 16ï¼‰
    for region, nodes in region_results.items():
        top_nodes = nodes[:MAX_OUTPUT_PER_REGION]
        
        with open(f"{OUTPUT_DIR}/ip_{region}.txt", "w") as f:
            for n in top_nodes:
                f.write(f'{n["ip"]}:{n["port"]}#{region}-score{n["score"]}\n')
        
        logging.info(f"{region}: ä¿å­˜ {len(top_nodes)} ä¸ªèŠ‚ç‚¹")
    
    # ä¿å­˜ JSON
    with open(f"{OUTPUT_DIR}/ip_candidates.json", "w") as f:
        json.dump({
            "meta": {
                "generated_at": datetime.utcnow().isoformat() + "Z",
                "total_nodes": len(all_nodes),
                "regions": {r: len(nodes) for r, nodes in region_results.items()}
            },
            "nodes": all_nodes[:200]  # åªä¿å­˜å‰200ä¸ª
        }, f, indent=2)
    
    # æ‰“å°ç»Ÿè®¡
    print("\n" + "="*60)
    print("ğŸ“Š æ‰«æç»Ÿè®¡")
    print("="*60)
    for region in sorted(region_results.keys()):
        nodes = region_results[region]
        if nodes:
            avg_score = sum(n["score"] for n in nodes) / len(nodes)
            print(f"{region:4s}: {len(nodes):3d} èŠ‚ç‚¹ | å¹³å‡åˆ†æ•°: {avg_score:.3f}")
    print("="*60)

    logging.info("\nâœ… æ‰«æå®Œæˆï¼")

if __name__ == "__main__":
    main()