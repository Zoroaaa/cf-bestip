import subprocess
import random
import ipaddress
import requests
import os
import json
import time
import logging
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime

# =========================
# é…ç½®æ—¥å¿—
# =========================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[logging.StreamHandler()]
)

# =========================
# åŸºç¡€å‚æ•°
# =========================

CF_IPS_V4_URL = "https://www.cloudflare.com/ips-v4"

TRACE_DOMAINS = {
    "v0": "sptest.ittool.pp.ua",
    "v1": "sptest1.ittool.pp.ua",
    "v2": "sptest2.ittool.pp.ua",
}

SAMPLE_SIZE = 800
TIMEOUT = 8
CONNECT_TIMEOUT = 4
MAX_WORKERS = 12
LATENCY_LIMIT = 800

OUTPUT_DIR = "public"
DATA_DIR = "public/data"
PROXY_CACHE_DIR = "proxy_cache"

HTTPS_PORTS = [443, 8443, 2053, 2083, 2087, 2096]

# ç›®æ ‡åœ°åŒºé…ç½®
REGION_CONFIG = {
    "HK": {"codes": ["HK"], "sample": 100},
    "SG": {"codes": ["SG"], "sample": 100},
    "JP": {"codes": ["JP"], "sample": 100},
    "KR": {"codes": ["KR"], "sample": 80},
    "TW": {"codes": ["TW"], "sample": 80},
    "US": {"codes": ["US"], "sample": 120},
    "DE": {"codes": ["DE"], "sample": 60},
    "UK": {"codes": ["GB"], "sample": 60},
    "AU": {"codes": ["AU"], "sample": 60},
    "CA": {"codes": ["CA"], "sample": 60},
}

MAX_OUTPUT_PER_REGION = 32
GOOD_SCORE_THRESHOLD = 0.75
MAX_PROXIES_PER_REGION = 3  # å‡å°‘åˆ°3ä¸ª,æé«˜æˆåŠŸç‡

# ä»£ç†æµ‹è¯•é…ç½®
PROXY_TEST_TIMEOUT = 5  # ä»£ç†æµ‹è¯•è¶…æ—¶(ç§’)
PROXY_QUICK_TEST_URL = "http://www.gstatic.com/generate_204"  # ç”¨äºå¿«é€Ÿæµ‹è¯•
PROXY_MAX_LATENCY = 3000  # ä»£ç†æœ€å¤§å¯æ¥å—å»¶è¿Ÿ(æ¯«ç§’)

# =========================
# COLO â†’ Region
# =========================

COLO_MAP = {
    "HKG": "HK", "SIN": "SG", "NRT": "JP", "KIX": "JP",
    "ICN": "KR", "TPE": "TW",
    "SYD": "AU", "MEL": "AU",
    "LAX": "US", "SJC": "US", "SFO": "US",
    "SEA": "US", "ORD": "US", "DFW": "US",
    "ATL": "US", "IAD": "US", "EWR": "US",
    "JFK": "US", "BOS": "US", "MIA": "US",
    "PHX": "US", "DEN": "US", "IAH": "US",
    "FRA": "DE", "MUC": "DE", "AMS": "DE",
    "LHR": "UK", "LGW": "UK", "MAN": "UK",
    "YYZ": "CA", "YVR": "CA",
}

# =========================
# æ”¹è¿›çš„ä»£ç†è·å–å™¨
# =========================

class ProxyFetcher:
    """ä»å¤šä¸ªæºè·å–å¹¶éªŒè¯ä»£ç†"""
    
    def __init__(self, cache_dir=PROXY_CACHE_DIR):
        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
    
    def get_cache_path(self, region):
        return os.path.join(self.cache_dir, f"verified_proxies_{region}.json")
    
    def is_cache_valid(self, region, max_age=3600):  # 1å°æ—¶ç¼“å­˜
        cache_file = self.get_cache_path(region)
        if not os.path.exists(cache_file):
            return False
        age = time.time() - os.path.getmtime(cache_file)
        return age < max_age
    
    def load_from_cache(self, region):
        cache_file = self.get_cache_path(region)
        try:
            with open(cache_file, 'r') as f:
                data = json.load(f)
                logging.info(f"âœ“ ä»ç¼“å­˜åŠ è½½ {len(data)} ä¸ªå·²éªŒè¯çš„ {region} ä»£ç†")
                return data
        except:
            return []
    
    def save_to_cache(self, region, proxies):
        if not proxies:
            return
        cache_file = self.get_cache_path(region)
        with open(cache_file, 'w') as f:
            json.dump(proxies, f)
        logging.info(f"âœ“ ç¼“å­˜ {len(proxies)} ä¸ªå·²éªŒè¯ä»£ç†")
    
    def fetch_from_pubproxy(self, country_code):
        """PubProxy API - è¾ƒå¯é çš„å…è´¹æº"""
        proxies = []
        try:
            url = f"http://pubproxy.com/api/proxy?limit=20&format=json&type=http&country={country_code}"
            resp = self.session.get(url, timeout=10)
            if resp.status_code == 200:
                data = resp.json()
                for item in data.get('data', []):
                    proxies.append({
                        "host": item['ip'],
                        "port": int(item['port']),
                        "type": item.get('type', 'http'),
                        "country": country_code,
                        "source": "pubproxy"
                    })
        except Exception as e:
            logging.debug(f"PubProxy {country_code} å¤±è´¥: {e}")
        return proxies
    
    def fetch_from_proxylist_geonode(self, country_code):
        """Geonode - è´¨é‡è¾ƒå¥½"""
        proxies = []
        try:
            url = (
                f"https://proxylist.geonode.com/api/proxy-list"
                f"?limit=50&page=1&sort_by=lastChecked&sort_type=desc"
                f"&country={country_code}"
                f"&protocols=http,https"
                f"&filterUpTime=90"  # åªè¦90%+åœ¨çº¿ç‡
            )
            resp = self.session.get(url, timeout=10)
            if resp.status_code == 200:
                data = resp.json()
                for item in data.get('data', [])[:30]:
                    proxies.append({
                        "host": item['ip'],
                        "port": int(item['port']),
                        "type": 'http',
                        "country": country_code,
                        "source": "geonode",
                        "uptime": item.get('upTime', 0)
                    })
        except Exception as e:
            logging.debug(f"Geonode {country_code} å¤±è´¥: {e}")
        return proxies
    
    def fetch_from_proxy11(self):
        """Proxy11 - å¤‡ç”¨æº"""
        proxies = []
        try:
            url = "https://api.proxy11.com/api/proxy-list?limit=100"
            resp = self.session.get(url, timeout=10)
            if resp.status_code == 200:
                data = resp.json()
                for item in data.get('proxies', [])[:50]:
                    proxies.append({
                        "host": item['ip'],
                        "port": int(item['port']),
                        "type": 'http',
                        "country": item.get('country', 'UNKNOWN'),
                        "source": "proxy11"
                    })
        except Exception as e:
            logging.debug(f"Proxy11 å¤±è´¥: {e}")
        return proxies
    
    def fetch_from_github_proxy_list(self):
        """GitHubä»£ç†åˆ—è¡¨ - ç¤¾åŒºç»´æŠ¤"""
        proxies = []
        try:
            # TheSpeedX/PROXY-List
            url = "https://raw.githubusercontent.com/TheSpeedX/PROXY-List/master/http.txt"
            resp = self.session.get(url, timeout=10)
            if resp.status_code == 200:
                for line in resp.text.strip().split('\n')[:100]:
                    if ':' in line:
                        try:
                            host, port = line.strip().split(':')
                            proxies.append({
                                "host": host,
                                "port": int(port),
                                "type": "http",
                                "country": "UNKNOWN",
                                "source": "github"
                            })
                        except:
                            pass
        except Exception as e:
            logging.debug(f"GitHubä»£ç†åˆ—è¡¨å¤±è´¥: {e}")
        return proxies
    
    def fetch_all_sources(self, country_codes):
        """ä»æ‰€æœ‰æºè·å–ä»£ç†"""
        all_proxies = []
        
        # 1. ä¼˜å…ˆè·å–æŒ‡å®šå›½å®¶ä»£ç†
        for country_code in country_codes:
            logging.info(f"  â†’ è·å– {country_code} ä»£ç†...")
            
            # æ¥æº1: PubProxy
            proxies = self.fetch_from_pubproxy(country_code)
            all_proxies.extend(proxies)
            time.sleep(0.3)
            
            # æ¥æº2: Geonode
            proxies = self.fetch_from_proxylist_geonode(country_code)
            all_proxies.extend(proxies)
            time.sleep(0.3)
        
        # 2. é€šç”¨ä»£ç†æº(ä½œä¸ºè¡¥å……)
        if len(all_proxies) < 20:
            logging.info("  â†’ è·å–é€šç”¨ä»£ç†...")
            all_proxies.extend(self.fetch_from_proxy11())
            all_proxies.extend(self.fetch_from_github_proxy_list())
        
        # å»é‡
        unique_proxies = []
        seen = set()
        for p in all_proxies:
            key = f"{p['host']}:{p['port']}"
            if key not in seen:
                seen.add(key)
                unique_proxies.append(p)
        
        logging.info(f"  âœ“ è·å–åˆ° {len(unique_proxies)} ä¸ªä¸é‡å¤ä»£ç†")
        return unique_proxies
    
    def get_proxies(self, region):
        """è·å–æŒ‡å®šåœ°åŒºçš„ä»£ç†"""
        
        # æ£€æŸ¥ç¼“å­˜
        if self.is_cache_valid(region):
            cached = self.load_from_cache(region)
            if len(cached) >= 3:
                return cached
        
        country_codes = REGION_CONFIG.get(region, {}).get("codes", [])
        if not country_codes:
            logging.warning(f"æœªæ‰¾åˆ° {region} çš„å›½å®¶ä»£ç ")
            return []
        
        logging.info(f"\n{'='*50}")
        logging.info(f"è·å– {region} åœ°åŒºä»£ç†...")
        logging.info(f"{'='*50}")
        
        # è·å–åŸå§‹ä»£ç†åˆ—è¡¨
        all_proxies = self.fetch_all_sources(country_codes)
        
        if not all_proxies:
            logging.warning(f"âš  {region} æœªè·å–åˆ°ä»»ä½•ä»£ç†")
            return []
        
        return all_proxies

# =========================
# ä»£ç†å¿«é€ŸéªŒè¯
# =========================

def quick_test_proxy(proxy, test_url=PROXY_QUICK_TEST_URL):
    """å¿«é€Ÿæµ‹è¯•ä»£ç†è¿é€šæ€§"""
    try:
        proxy_url = f"{proxy.get('type', 'http')}://{proxy['host']}:{proxy['port']}"
        proxies_dict = {"http": proxy_url, "https": proxy_url}
        
        start = time.time()
        resp = requests.get(
            test_url,
            proxies=proxies_dict,
            timeout=PROXY_TEST_TIMEOUT,
            allow_redirects=False
        )
        latency = int((time.time() - start) * 1000)
        
        # 204 No Content æˆ– 200 éƒ½ç®—æˆåŠŸ
        if resp.status_code in [200, 204] and latency < PROXY_MAX_LATENCY:
            proxy['test_latency'] = latency
            return True
    except:
        pass
    return False

def test_proxy_with_cloudflare(proxy):
    """ç”¨Cloudflare traceæµ‹è¯•ä»£ç†å¹¶è·å–ä½ç½®"""
    try:
        proxy_url = f"{proxy.get('type', 'http')}://{proxy['host']}:{proxy['port']}"
        proxies_dict = {"http": proxy_url, "https": proxy_url}
        
        resp = requests.get(
            "https://cloudflare.com/cdn-cgi/trace",
            proxies=proxies_dict,
            timeout=PROXY_TEST_TIMEOUT,
            verify=False
        )
        
        if resp.status_code == 200:
            for line in resp.text.split('\n'):
                if line.startswith('colo='):
                    proxy['colo'] = line.split('=')[1].strip().upper()
                elif line.startswith('loc='):
                    proxy['loc'] = line.split('=')[1].strip().upper()
            return True
    except:
        pass
    return False

def filter_working_proxies(proxies, max_workers=30, max_proxies=MAX_PROXIES_PER_REGION):
    """ä¸¤é˜¶æ®µç­›é€‰ä»£ç†: 1.å¿«é€Ÿè¿é€šæ€§æµ‹è¯• 2.CloudflareéªŒè¯"""
    
    if not proxies:
        return []
    
    logging.info(f"\né˜¶æ®µ1: å¿«é€Ÿæµ‹è¯• {len(proxies)} ä¸ªä»£ç†è¿é€šæ€§...")
    
    # é˜¶æ®µ1: å¿«é€Ÿè¿é€šæ€§æµ‹è¯•
    quick_pass = []
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_proxy = {executor.submit(quick_test_proxy, p): p for p in proxies}
        
        for future in as_completed(future_to_proxy):
            proxy = future_to_proxy[future]
            try:
                if future.result(timeout=PROXY_TEST_TIMEOUT + 1):
                    quick_pass.append(proxy)
                    if len(quick_pass) >= max_proxies * 3:  # å¤šç­›é€‰ä¸€äº›å¤‡ç”¨
                        break
            except:
                pass
    
    # æŒ‰å»¶è¿Ÿæ’åº
    quick_pass.sort(key=lambda x: x.get('test_latency', 9999))
    logging.info(f"  âœ“ é€šè¿‡å¿«é€Ÿæµ‹è¯•: {len(quick_pass)} ä¸ª")
    
    if not quick_pass:
        return []
    
    # é˜¶æ®µ2: CloudflareéªŒè¯(åªæµ‹è¯•å‰é¢çš„)
    logging.info(f"\né˜¶æ®µ2: Cloudflareä½ç½®éªŒè¯...")
    working = []
    
    with ThreadPoolExecutor(max_workers=15) as executor:
        future_to_proxy = {
            executor.submit(test_proxy_with_cloudflare, p): p 
            for p in quick_pass[:max_proxies * 2]
        }
        
        for future in as_completed(future_to_proxy):
            if len(working) >= max_proxies:
                break
            
            proxy = future_to_proxy[future]
            try:
                if future.result(timeout=PROXY_TEST_TIMEOUT + 2):
                    working.append(proxy)
                    logging.info(
                        f"  âœ“ {proxy['host']}:{proxy['port']} "
                        f"[{proxy.get('colo', 'N/A')}] "
                        f"{proxy.get('test_latency', 0)}ms"
                    )
            except:
                pass
    
    logging.info(f"\nâœ“ æœ€ç»ˆå¯ç”¨: {len(working)} ä¸ªä»£ç†\n")
    return working

# =========================
# IPæµ‹è¯•(é€šè¿‡ä»£ç†æˆ–ç›´è¿)
# =========================

def curl_test_with_proxy(ip, domain, proxy=None):
    """ä½¿ç”¨ä»£ç†æµ‹è¯• Cloudflare IP"""
    try:
        cmd = ["curl", "-k", "-o", "/dev/null", "-s"]
        
        # æ·»åŠ ä»£ç†
        if proxy:
            proxy_type = proxy.get('type', 'http')
            if proxy_type == 'socks5':
                cmd.extend(["--socks5", f"{proxy['host']}:{proxy['port']}"])
            else:
                cmd.extend(["-x", f"{proxy['host']}:{proxy['port']}"])
        
        cmd.extend([
            "-w", "%{time_connect} %{time_appconnect} %{http_code}",
            "--http1.1",
            "--connect-timeout", str(CONNECT_TIMEOUT),
            "--max-time", str(TIMEOUT),
            "--resolve", f"{domain}:443:{ip}",
            f"https://{domain}"
        ])
        
        out = subprocess.check_output(cmd, timeout=TIMEOUT + 2, stderr=subprocess.DEVNULL)
        parts = out.decode().strip().split()
        
        if len(parts) < 3:
            return None
        
        tc, ta, code = parts[0], parts[1], parts[2]
        latency = int((float(tc) + float(ta)) * 1000)
        
        if latency > LATENCY_LIMIT or code in ["000", "0"]:
            return None
        
        # è·å– CF-Ray
        hdr_cmd = ["curl", "-k", "-sI"]
        
        if proxy:
            proxy_type = proxy.get('type', 'http')
            if proxy_type == 'socks5':
                hdr_cmd.extend(["--socks5", f"{proxy['host']}:{proxy['port']}"])
            else:
                hdr_cmd.extend(["-x", f"{proxy['host']}:{proxy['port']}"])
        
        hdr_cmd.extend([
            "--resolve", f"{domain}:443:{ip}",
            f"https://{domain}"
        ])
        
        hdr = subprocess.check_output(
            hdr_cmd,
            timeout=TIMEOUT,
            stderr=subprocess.DEVNULL
        ).decode(errors="ignore").lower()
        
        ray = None
        for line in hdr.splitlines():
            if line.startswith("cf-ray"):
                ray = line.split(":")[1].strip()
                break
        
        if not ray:
            return None
        
        colo = ray.split("-")[-1].upper()
        region = COLO_MAP.get(colo, "UNMAPPED")
        
        return {
            "ip": str(ip),
            "domain": domain,
            "colo": colo,
            "region": region,
            "latency": latency,
            "proxy": f"{proxy['host']}:{proxy['port']}" if proxy else "direct"
        }
    
    except Exception:
        return None

def test_ip_with_proxy(ip, proxy=None):
    """æµ‹è¯•å•ä¸ª IPï¼ˆå¤šä¸ªåŸŸåï¼‰"""
    records = []
    for view, domain in TRACE_DOMAINS.items():
        r = curl_test_with_proxy(ip, domain, proxy)
        if r:
            r["view"] = view
            records.append(r)
    return records

# =========================
# å·¥å…·å‡½æ•°
# =========================

def fetch_cf_ipv4_cidrs():
    r = requests.get(CF_IPS_V4_URL, timeout=10)
    r.raise_for_status()
    return [x.strip() for x in r.text.splitlines() if x.strip()]

def weighted_random_ips(cidrs, total):
    pools = []
    for c in cidrs:
        net = ipaddress.ip_network(c)
        pools.append((net, net.num_addresses))
    
    total_weight = sum(w for _, w in pools)
    result = []
    
    for net, weight in pools:
        cnt = max(1, int(total * weight / total_weight))
        hosts = list(net.hosts())
        if hosts:
            result.extend(random.sample(hosts, min(cnt, len(hosts))))
    
    random.shuffle(result)
    return result[:total]

def score_ip(latencies):
    if len(latencies) < 2:
        return 0
    
    lat_min = min(latencies)
    lat_max = max(latencies)
    
    s_stability = len(latencies) / len(TRACE_DOMAINS)
    s_consistency = max(0.3, 1 - (lat_max - lat_min) / LATENCY_LIMIT)
    s_latency = 1 / (1 + lat_min / 100)
    
    return round(s_stability * s_consistency * s_latency, 4)

def aggregate_nodes(raw):
    ip_map = defaultdict(list)
    for r in raw:
        ip_map[r["ip"]].append(r)
    
    nodes = []
    for ip, items in ip_map.items():
        latencies = [x["latency"] for x in items]
        score = score_ip(latencies)
        if score <= 0:
            continue
        
        best = min(items, key=lambda x: x["latency"])
        nodes.append({
            "ip": ip,
            "port": random.choice(HTTPS_PORTS),
            "region": best["region"],
            "colo": best["colo"],
            "latencies": latencies,
            "score": score
        })
    
    return nodes

# =========================
# åˆ†åœ°åŒºæ‰«æ(æ”¹è¿›ç­–ç•¥)
# =========================

def scan_region(region, ips, proxies):
    """æ‰«ææŒ‡å®šåœ°åŒº - ä¼˜å…ˆä»£ç†,é™çº§åˆ°ç›´è¿"""
    logging.info(f"\n{'='*60}")
    logging.info(f"å¼€å§‹æ‰«æåœ°åŒº: {region}")
    logging.info(f"{'='*60}")
    
    raw_results = []
    
    # ç­–ç•¥1: å¦‚æœæœ‰ä»£ç†,ä¼˜å…ˆä½¿ç”¨ä»£ç†
    if proxies:
        logging.info(f"ä½¿ç”¨ {len(proxies)} ä¸ªä»£ç†è¿›è¡Œæ‰«æ...")
        
        ips_per_proxy = max(1, len(ips) // len(proxies))
        
        for i, proxy in enumerate(proxies):
            proxy_ips = ips[i*ips_per_proxy:(i+1)*ips_per_proxy]
            
            if not proxy_ips:
                continue
            
            proxy_info = f"{proxy['host']}:{proxy['port']} [{proxy.get('colo', 'N/A')}]"
            logging.info(f"  â†’ é€šè¿‡ä»£ç† {proxy_info} æµ‹è¯• {len(proxy_ips)} ä¸ªIP...")
            
            with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
                futures = [executor.submit(test_ip_with_proxy, ip, proxy) for ip in proxy_ips]
                
                for future in as_completed(futures):
                    try:
                        batch = future.result(timeout=TIMEOUT + 5)
                        if batch:
                            raw_results.extend(batch)
                    except:
                        pass
        
        logging.info(f"  âœ“ ä»£ç†æ‰«ææ”¶é›†: {len(raw_results)} æ¡ç»“æœ")
    
    # ç­–ç•¥2: å¦‚æœä»£ç†ç»“æœå¤ªå°‘,è¡¥å……ç›´è¿æ‰«æ
    if len(raw_results) < len(ips) * 0.3:  # å¦‚æœç»“æœå°‘äº30%
        logging.info(f"âš  ä»£ç†ç»“æœä¸è¶³,ä½¿ç”¨ç›´è¿è¡¥å……æ‰«æ...")
        
        # ä½¿ç”¨å‰©ä½™IPæˆ–å…¨éƒ¨IP
        remaining_ips = ips if not raw_results else ips[:len(ips)//2]
        
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            futures = [executor.submit(test_ip_with_proxy, ip, None) for ip in remaining_ips]
            
            for future in as_completed(futures):
                try:
                    batch = future.result(timeout=TIMEOUT + 5)
                    if batch:
                        raw_results.extend(batch)
                except:
                    pass
        
        logging.info(f"  âœ“ ç›´è¿è¡¥å……æ”¶é›†: {len(raw_results)} æ¡æ€»ç»“æœ")
    
    logging.info(f"âœ“ {region}: æ€»è®¡æ”¶é›† {len(raw_results)} æ¡æµ‹è¯•ç»“æœ\n")
    return raw_results

# =========================
# ä¸»æµç¨‹
# =========================

def main():
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    os.makedirs(DATA_DIR, exist_ok=True)
    
    logging.info(f"\n{'#'*60}")
    logging.info(f"# Cloudflare IP ä¼˜é€‰æ‰«æå™¨ (æ”¹è¿›ç‰ˆ)")
    logging.info(f"# ç­–ç•¥: ä»£ç†ä¼˜å…ˆ â†’ ç›´è¿é™çº§")
    logging.info(f"{'#'*60}\n")
    
    # åˆå§‹åŒ–ä»£ç†è·å–å™¨
    proxy_fetcher = ProxyFetcher()
    
    # è·å– Cloudflare IP æ®µ
    logging.info("è·å– Cloudflare IP èŒƒå›´...")
    cidrs = fetch_cf_ipv4_cidrs()
    
    # ç”Ÿæˆæµ‹è¯• IP æ± 
    total_ips = sum(cfg["sample"] for cfg in REGION_CONFIG.values())
    logging.info(f"ç”Ÿæˆ {total_ips} ä¸ªæµ‹è¯• IP...\n")
    all_test_ips = weighted_random_ips(cidrs, total_ips)
    
    # æŒ‰åœ°åŒºåˆ†é… IP å¹¶æ‰«æ
    all_results = []
    region_results = {}
    
    ip_offset = 0
    for region, config in REGION_CONFIG.items():
        sample_size = config["sample"]
        region_ips = all_test_ips[ip_offset:ip_offset + sample_size]
        ip_offset += sample_size
        
        # è·å–è¯¥åœ°åŒºçš„ä»£ç†
        raw_proxies = proxy_fetcher.get_proxies(region)
        
        # éªŒè¯ä»£ç†
        working_proxies = filter_working_proxies(raw_proxies)
        
        # ä¿å­˜å·²éªŒè¯ä»£ç†åˆ°ç¼“å­˜
        if working_proxies:
            proxy_fetcher.save_to_cache(region, working_proxies)
        
        # æ‰«æ
        raw = scan_region(region, region_ips, working_proxies)
        nodes = aggregate_nodes(raw)
        
        region_results[region] = nodes
        all_results.extend(raw)
        
        logging.info(f"{'='*60}")
        logging.info(f"âœ“ {region}: å‘ç° {len(nodes)} ä¸ªæœ‰æ•ˆèŠ‚ç‚¹")
        logging.info(f"{'='*60}\n")
        
        # é¿å…è¯·æ±‚è¿‡å¿«
        time.sleep(1)
    
    # æ±‡æ€»æ‰€æœ‰èŠ‚ç‚¹
    all_nodes = aggregate_nodes(all_results)
    all_nodes.sort(key=lambda x: x["score"], reverse=True)
    
    logging.info(f"\n{'='*60}")
    logging.info(f"æ€»è®¡å‘ç° {len(all_nodes)} ä¸ªèŠ‚ç‚¹")
    logging.info(f"{'='*60}\n")
    
    # ä¿å­˜æ€»æ–‡ä»¶
    all_lines = [f'{n["ip"]}:{n["port"]}#{n["region"]}-score{n["score"]}\n' for n in all_nodes]
    
    with open(f"{OUTPUT_DIR}/ip_all.txt", "w") as f:
        f.writelines(all_lines)
    
    # ä¿å­˜å†å²
    today = datetime.utcnow().strftime("%Y-%m-%d")
    with open(f"{DATA_DIR}/ip_all_{today}.txt", "w") as f:
        f.writelines(all_lines)
    
    # æ¸…ç†æ—§å†å²
    history_files = sorted([f for f in os.listdir(DATA_DIR) if f.startswith("ip_all_")])
    while len(history_files) > 7:
        os.remove(os.path.join(DATA_DIR, history_files.pop(0)))
    
    # ä¿å­˜é«˜è´¨é‡æ± 
    good_pool = [n for n in all_nodes if n["score"] >= GOOD_SCORE_THRESHOLD]
    with open(f"{OUTPUT_DIR}/ip_good_pool.txt", "w") as f:
        for n in good_pool:
            f.write(f'{n["ip"]}:{n["port"]}#{n["region"]}-score{n["score"]}\n')
    
    # æŒ‰åœ°åŒºä¿å­˜
    for region, nodes in region_results.items():
        nodes.sort(key=lambda x: x["score"], reverse=True)
        top_nodes = nodes[:MAX_OUTPUT_PER_REGION]
        
        with open(f"{OUTPUT_DIR}/ip_{region}.txt", "w") as f:
            for n in top_nodes:
                f.write(f'{n["ip"]}:{n["port"]}#{region}-score{n["score"]}\n')
        
        logging.info(f"{region}: ä¿å­˜ {len(top_nodes)} ä¸ªèŠ‚ç‚¹")
    
    # ä¿å­˜ JSON
    with open(f"{OUTPUT_DIR}/ip_candidates.json", "w") as f:
        json.dump({
            "meta": {
                "generated_at": datetime.utcnow().isoformat() + "Z",
                "total_nodes": len(all_nodes),
                "regions": {r: len(nodes) for r, nodes in region_results.items()}
            },
            "nodes": all_nodes[:200]  # åªä¿å­˜å‰200ä¸ª
        }, f, indent=2)
    
    
    # æ‰“å°ç»Ÿè®¡
    print("\n" + "="*60)
    print("ğŸ“Š æ‰«æç»Ÿè®¡")
    print("="*60)
    for region in sorted(region_results.keys()):
        nodes = region_results[region]
        if nodes:
            avg_score = sum(n["score"] for n in nodes) / len(nodes)
            print(f"{region:4s}: {len(nodes):3d} èŠ‚ç‚¹ | å¹³å‡åˆ†æ•°: {avg_score:.3f}")
    print("="*60)

    logging.info("\nâœ… æ‰«æå®Œæˆï¼")

if __name__ == "__main__":
    main()
